{"pages":[],"posts":[{"title":"[Python] 파이썬으로 간단한 테스트 데이터 생성하기 (csv)","text":"테스트를 위한 데이터가 필요해서 데이터를 만들어야 할 때가 있다. 적은 양의 데이터는 수동으로 일일히 만들수도 있지만 데이터 양이 많아지면(1000건만 넘어가도…) 하나씩 만드는 건 너무 노가다. 그럴 때 데이터가 복잡하지 않고 단순한 구성일 경우에는 파이썬을 이용해 간단하게 생성할 수 있다. random 모듈을 이용해 간단하게 짠 코드아래의 코드는 int형 데이터 3개, varchar 형 13개, datetime형 4개로 이루어진 레코드 1000개 를 만들어서 csv 파일로 저장한다. 1import csv2import random3import datetime4import timeit5import argparse678class Argparser:9 def __init__(self):10 self.parser = argparse.ArgumentParser()11 self.init_parser()1213 def init_parser(self):14 parser = self.parser15 parser.add_argument(\"-i\", \"--integers\", help=\"how many integer value do you want to generate\")16 parser.add_argument(\"-ri\", \"--range-integer\", help=\"describe range of integer\")17 parser.add_argument(\"-vc\", \"--varchars\", help=\"how many varchar value do you want to generate\")18 parser.add_argument(\"-t\", \"--texts\", help=\"how many text value do you want to generate\")19 parser.add_argument(\"-dt\", \"--dates\", help=\"how many datetime value do you want to generate\")20 parser.add_argument(\"-d\", \"--doubles\", help=\"how many double value do you want to generate\")21 parser.add_argument(\"-r\", \"--rows\", help=\"describe number of rows\")22 parser.add_argument(\"-ta\", \"--textarrays\", help=\"how many text arrays...\")23 parser.add_argument(\"-o\", \"--output\", help=\"describe file name\")24 self.parser = parser252627class Generator:28 def __init__(self, args):29 self.rows = int(args.rows) if args.rows else 100030 self.filename = args.output if args.output else \"out.csv\"31 self.integers = int(args.integers) if args.integers else 532 self.varchars = int(args.varchars) if args.varchars else 533 self.texts = int(args.texts) if args.texts else 534 self.doubles = int(args.doubles) if args.doubles else 535 self.dates = int(args.dates) if args.dates else 136 self.textarrays = int(args.textarrays) if args.textarrays else 037 self.text_pool = ['Amazon Elastic Compute Cloud (Amazon EC2)',38 'First you need to get set up to use Amazon EC2.', 39 'You can provision Amazon EC2 resources such as instances and volumes.',40 'If you prefer to build applications using language-sp.']41 self.word_pool = self.text_pool[0].split()4243 def generate(self):44 f = open(self.filename, 'w', encoding='utf-8', newline='')45 wr = csv.writer(f)46 for i in range(0, self.rows):47 idx = [i + 1]48 val_ints = []49 val_varchars = []50 val_texts = []51 val_doubles = []52 val_dates = []53 val_textarrays = []5455 for j in range(0, self.integers):56 val_ints.append(random.randrange(0, 10000))57 for j in range(0, self.varchars):58 val_varchars.append(random.choice(self.word_pool))59 for j in range(0, self.texts):60 val_texts.append(random.choice(self.text_pool))61 for j in range(0, self.textarrays):62 val_textarrays.append('{\\\"abc\\\",\\\"def\\\",\\\"ghi\\\"}')63 for j in range(0, self.dates):64 val_dates.append(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'))65 for j in range(0, self.doubles):66 val_doubles.append(random.random())67 wr.writerow(idx + val_ints + val_varchars + val_texts + val_textarrays + val_dates + val_doubles)68 f.close()697071if __name__ == \"__main__\":72 parser = Argparser()73 args = parser.parser.parse_args()74 print(\"Generating file...\")75 start = timeit.default_timer()76 generator = Generator(args)77 generator.generate()78 stop = timeit.default_timer()79 print(generator.filename + \" is created. \")80 print(\"It took \" + str(stop - start) + \" seconds to generate data.\") 간단하게 설명하자면…각 데이터에 맞는 값을 random모듈을 통해 생성하고 csv형태에 맞게 출력한다. random.randrange()를 이용해 int를 만들고 random.choice()를 통해 문자열을 만들고 datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') 을 이용해 현재 시간을 스트링으로 만든다 너무 쉬워서 설명이 이게 끝이다. 여기에 코드를 추가해서 데이터에 변화를 줄 수도 있다. random모듈에 내장되어 있는 정규분포함수를 이용해 정규분포에 따르는 데이터를 만들수도 있고 랜덤함수 여러개를 사용해서 값을 좀더 풍성하게 바꿀 수도 있다. ### 사이드 프로젝트...? 데이터 생성기를 만드는 건 간단하지만 활용도는 꽤 높은 것 같다. 위 코드는 그냥 하드 코딩이어서 재사용하려면 일일이 수정해 줘야 한다. 그런데 앞으로 쓸 일이 많아서 재사용할수 있는 형태로 만들어 보려고 한다. 커맨드라인으로 커스터마이징이 가능한 파이썬 프로그램 웹을 통해 만들고자 하는 데이터의 정보/포맷을 입력하면 csv파일을 다운로드를 받을 수 있는 서비스 2번의 경우 고민할 것이 많아지겠지만(연산을 어디에서 할 것인지, 서버를 둘 것인지, 어떤 언어를 사용할 것인지 등등…) 사이드 프로젝트로 적당한 난이도와 규모가 아닐까 싶다. 필요한 사람도 꽤 있을 것 같고… #### 추가사항... 데이터를 만들다보니 많은 데이터를 만들경우 위 코드를 사용하면 시간이 너무 오래 걸린다(20개 컬럼 1억 5천만 줄 생성시 1시간 30분 이상 소요). cpu사양도 높으면 좋겠지만 파이썬은 기본적으로 한개의 쓰레드에서만 동작하니 아무리 cpu의 코어 수, 쓰레드 수가 많아도 쓰질 않으니 소용이 없다. 그래서 쓰레딩을 도입해보려고 한다. 다음 포스팅에서는 간단하게 쓰레딩을 도입해서 쓰레딩 도입 한 것과 안 한 것을 비교해 봐야겠다. + 만약에 이 코드를 c나 Java, go등의 언어로 짜면 성능이 얼마나 개선될까?","link":"/2018/07/05/python-generate-simple-data-in-csv/"},{"title":"[Jekyll] Jekyll, Github page 를 이용해서 테크 블로그를 만들어 보자 - 1. Github page","text":"사실 인터넷에 github page와 jekyll을 이용한 블로그 만들기에 관한 글은 많다. 근데 생각보다 각각의 연동이 잘 정리되어 있는 글은 없는 것 같아서 정리할 겸 올려본다. 목차 github page jekyll google analytics disqus 이 블로그는 gitgub page와 jekyll이라는 것을 이용해서 만들었다. 네이버, 티스토리 등이 간편하게 블로그를 만들 수 있게 서비스해주고 있지만 네이버의 경우 커스터마이징이 힘들고 결정적으로 구글에서 검색 노출이 안된다. 티스토리를 많이들 이용하지만 초대권 받는것도 귀찮고 왠지 뭔가 다르고 힙…한 것은 없을까 찾아보다가 github에서 정적 웹페이지를 호스팅해주는 github page라는 서비스를 이용해보기로 했다. github page깃허브는 많이 들어봤을 것이다. 깃헙페이지는 그 깃허브에서 제공하는 정적 웹페이지 호스팅 서비스이다. 정적으로 웹페이지를 올리면 그것들을 인터넷 상에 게시해 주는 서비스이다. 깃허브 페이지를 이용하면 정적인 웹페이지를 무료로 호스팅할 수 있고 *.github.io라는 힙한 도메인 주소를 얻을 수 있다. 다음 순서를 따라가면서 간단하게 깃헙페이지를 어떻게 이용하는지 알아보자. 깃헙 페이지 만들기 index.html 사전 요구사항 깃허브 계정 …….. 까지 작성했으나 이것 역시 나보다 잘 정리하신 분들이 있다. 링크로 대체한다.깃헙페이지 만들기: http://blog.saltfactory.net/create-personal-web-site-using-with-github-pages/","link":"/2018/07/09/blogging-with-jekyll-and-github-page-1/"},{"title":"SAML에 대한 매우 개략적인 정리","text":"SAML 이란?Security Asserting Markup Language 즉, 보안 인증에 관한 정보를 기술하는 마크업 언어 정도로 생각하면 될 것 같다. 혹은 인증/인가 정보를 담은 XML 정도로 간단하게 생각하면 될 것 같다. 그렇다면 SAML은 어디에 쓰이는가?SSO(Single Sign-On)을 구현하기 위해 쓰인다. SSO는 한 번의 로그인으로 여러개의 다른 도메인을 이용하려고 할때 필요한 기술이다. 예를 들어, 온프레미스에 이미 구축해 놓은 인증 시스템을 이용해서 AWS 리소스에 접근하려고 할때 AWS에 별도의 인증 시스템을 구축하지 않고 온프레미스의 인증을 그대로 쓰려고 할 때 사용한다. SAML을 통한 인증 프로세스SAML 인증과정을 잘 설명한 블로그 글들이 있어 소개한다. 간략하게 설명한 인증 과정[과정1] Browser에서 사이트 SpA로 접속한다. 사이트 SpA에는 로그인이 되어 있지 않기 때문에 (세션이 없어서). SpA에서는 SAML request를 만들어서, Browser에게로 redirect URL을 보낸다. Browser는 redirect URL에 따라 IdP에 접속하고, Idp에서 login form을 넣고 log in을 한다. 이때, IdP와 Brower 사이에 HttpSession 또는 Cookie로 Login에 대한 정보를 기록한다. 그리고 다시 사이트 SpA로의 SAML response를 포함한 redirect URL을 browser로 전송한다. Browser는 SAML reponse를 가지고 SpA로 접속하면, SpA에는 인증된 정보를 가지고 로그인 처리를 한다. ※ 이 과정에서는 바로 사이트 SpA의 사용자 페이지(예를 들어 /home)등으로 가는 것이 아니라, SAML에 의해서 미리 정의한 SpA의 SAML response 처리 URL로 갔다가 SAML response를 처리가 끝나면 인증 처리를 한후, 사용자 페이지(/home)으로 다시 redirect한다. [과정2] 사이트 SpA에서 로그인된 상태에서 SpB에 접속한다. 사이트 SpB는 로그인이 되어 있지 않기 때문에, SAML 메시지를 만들어서 IdP의 login from으로 redirect URL을 보낸다. 브라우져는 redirect URL을 따라서 IdP에 접속을 한다. IdP에 접속을 하면 앞의 과정에서 이미 Session 또는 Cookie가 만들어져 있기 때문에 별도의 로그인 폼을 띄위지 않고, SAML response message와 함께, SpB로의 redirect URL을 전송한다. Browser는 Sp B에 인증된 정보를 가지고 로그인한다. (출처: http://bcho.tistory.com/755?category=481504 [조대협의 블로그]) 조금 더 디테일한 버전은 다음과 같다. 유저는 서비스 제공자(Service Provider)에게 접근한다. (서비스 이용을 위하여) 서비스 제공자는 SAMLRequest를 생성한다. 생성된 SAMLRequest은 XML format의 텍스트로 구성된다. 유저의 브라우저를 이용하여 인증 제공자(Identity Provider)로 Redirect 한다. 인증 제공자(Identity Provider)는 SAMLRequest를 파싱하고 유저인증을 진행한다. 인증제공자(Identity Provider)는 SAML Response를 생성한다. 인증제공자(Identity Provider)는 유저 브라우저를 이용하여 SAMLResponse data를 ACS로 전달한다. ACS는 Service Provider가 운영하게 되는데 SAMLResponse를 확인하고 실제 서비스 사이트로 유저를 Forwarding한다. 유저는 로그인에 성공하고 서비스를 제공받는다. (출처: http://civan.tistory.com/177 [행복만땅 개발자]) AWS와 연동은 아래 그림과 같은 과정으로 구현된다. 조직 내 사용자가 클라이언트 앱을 사용해 조직의 IdP로부터 인증을 요청합니다. IdP가 조직의 자격 증명 스토어를 이용하여 사용자를 인증합니다. IdP가 사용자에 대한 정보로 SAML 어설션을 만들어 클라이언트 앱으로 보냅니다. 클라이언트 앱이 AWS STS AssumeRoleWithSAML API를 호출하면서 SAML 공급자의 ARN, 수임할 역할의 ARN, IdP로부터 받은 SAML 어설션을 전달합니다. 클라이언트 앱에 대한 API 응답에는 임시 보안 자격 증명이 포함되어 있습니다. 클라이언트 앱은 임시 보안 자격 증명을 사용해 Amazon S3 API 작업을 호출합니다. (출처: https://docs.aws.amazon.com/ko_kr/IAM/latest/UserGuide/id_roles_providers_saml.html) saml이란 무엇인지 알기위해 매우 간략하고 기초적인 내용만 정리해봤다. 더 자세한 내용은 위에서 참조한 블로그에 자세히 나와있다. 특히 “행복만땅 개발자” 블로그에 올라온 포스트에서는 실습까지 해볼 수 있다.","link":"/2018/07/11/rough-draft-of-saml/"},{"title":"RDS(MySQL, MariaDB) - Multi-AZ / Read Replica (읽기전용 복제본) 관련 정리","text":"AWS RDS에는 가용성과 확장성을 위해 multi-az 라는 기능과 read replica라는 기능을 지원한다. 비슷하지만 다른 특징들을 갖고 있어 간단하게 정리해둔다. Multi-AZ Active - Standby 구조 자동 장애 복구 동기식 복제 (Synchronous replication) 위의 그림과 같이 원본 서버에 문제가 생기면 동기식으로 복제된 slave 인스턴스가 master로 자동으로 승격된다. 승격은 내부적으로 마스터를 바라보던 db instance의 도메인 엔드포인트가 slave를 가리킴으로써 구현된다. 읽기 복제본 생성과정 원본 스냅샷 생성 스냅샷을 통해 복제본 인스턴스 생성 변경사항 발생 시 비동기식 복제 기타 특징읽기복제본은 multi-az에 비해 자잘한 기능들이 많다. 순환복제 지원 X a에서 복제본 b를 만들고 b에서 다시 a로 복제하는 것이 불가능하다. a에서 b라는 복제본을 만들면 b에서는 다시 새로운 복제본(b’, b’’ 등)을 만드는 것은 가능하다. 체인식 복제 가능 (단 체인당 4대까지 )위에서 말했듯이 a에서 복제본 a’를 생성하고 a’에서 a’’를 생성하는 것이 가능하고 한 체인에 총 4대의 인스턴스까지 연결할 수 있다. 즉, a - a’ - a’’ - a’’’ 까지 가능하다는 말. mysql - binlog mariadb - gtid mysql은 복제시 원본에서 binarylog를 받아 복제본에다가 쓰는 작업을 통해 복제작업을 한다. 즉 복제본에서도 똑같이 쓰기 작업이 이루어져야 복제가 완료된다. mariadb는 gtid라는 방식을 사용한다고 한다. gtid가 어떤 방식인지는 다음에 알아보는걸로… read-only:0 으로 설정시 인덱스 설정등 쓰기 작업 가능 읽기복제본이지만 쓰기가 아예 안되는 것은 아닌가 보다. 읽기 복제본의 읽기 성능 향상 등을 위해 read-only 기능을 끌 수 있게 되어있다. 이 기능을 껐을 때 쓰기가 어디까지 되는지는 테스트 해봐야 알것 같다. 복제본 자체를 multi-az 로 이중화 구성 가능 말 그대로, 복제본 또한 multi-az 기능으로 slave노드를 만들어서 이중화 구성이 가능 외부에서 복제/ 외부로 복제 가능 온프레미스에 있는 db의 복제본을 rds에 올려 사용할 수 있고 반대로 rds에 있는 db를 온프레미스에 복제해서 사용할 수도 있다. 이중화 구성, DR 구성을 위한 옵션인 것 같다. 물론 부하 분산도 되고. Multi-AZ vs 읽기 복제본 AWS의 자료를 바탕으로 표로 정리해 보았다. Multi-AZ 읽기 복제본 동기식 복제 비동기식 복제 active-standby 읽기 부하 분산 secondary(standby)노드에서 복제 가능 기본값으로 백업 설정 x 최소 두개의 가용영역 단일리전/cross-AZ/cross-region 가능 업데이트 시 primary 에 적용 독립적으로 적용 가능 자동 장애 복구 장애 시 수동으로 승격 동기식 복제/ 비동기식 복제 차이점은 여기를 참고 참고자료 AWS RDS Documentation - 읽기복제본 작업 Deep Dive on Amazon RDS","link":"/2018/07/13/rds-multiaz-readreplica-summary/"},{"title":"자주쓰는 리눅스 명령어","text":"이 포스트는 계속 업데이트 될 예정입니다. ps -ef | grep python : 실행중인 프로세스 중 python을 포함한 프로세스를 보여준다 ls -lh : ls 명령어에 h 옵션을 붙이면 용량을 친절하게 표기해준다(mb, gb 등의 단위로 ) top : 실행중인 프로세스 중에 리소스를 많이 사용하는 프로세스들을 보여준다. sudo su : 슈퍼유저로 전환 df -h : 디스크별로 사용량을 보여준다. 앞서 말한 것처럼 h 옵션을 붙여주면 사람이 알아보기 편하게 바꿔준다. cat [파일이름] : 간단하지만 정말 많이 씀 tail -10 [파일이름] / tail -10f [파일이름] : 뒤에서 부터 읽을때 많이 씀 rm -rf : 앞뒤 생각없이 싹다 지울 때 -rf 옵션을 붙인다. nohup : 세션이 끊겨도 계속 지속되야하는 작업이 필요할때 쓴다.","link":"/2018/07/18/linux-commands/"},{"title":"자주쓰는 postgresql 쿼리","text":"set search_path to abc_schema : 스키마 설정. describe table_a; / \\d+ table_a : describe 하면 해당 테이블에 관련된 인덱스 까지 다 나와서 좋다. jdbc를 통한 클라이언트에서는 describe가 작동하지만 psql에서는 뒤의 명령어를 써야한다. drop index indexname; : 인덱스를 날려버린다. alter table postgre_c drop constraint postgre_c_pkey; : Primary key는 drop index 명령어로 안날아가서 이렇게 날려야 한다. ALTER TABLE distributors ADD PRIMARY KEY (dist_id); : primary key 추가할땐 이렇게. create index idx1 on table1 (column1) : 인덱스 추가는 이렇게 truncate table_a; : 테이블을 날려버릴땐 이렇게. 자동으로 vacuum까지 된다.","link":"/2018/07/21/postgre-sqls/"},{"title":"[docker] multihost networking - overlay","text":"여러 호스트를 이용하는 도커가 swarm을 이룰경우 서로 통신을 하려면 네트웍이 필요하다. 이때 각각 다른 호스트에서 생성된 컨테이너 사이에 네트워크를 생성해주는 것이 바로 overlay 네트워크 이다. 각각의 호스트에서 쓰고있는 네트워크와 상관없이 컨테이너 끼리의 사설망을 따로 만들어줌. 예를들어 각각의 호스트 A가 10.0.0.3 B가 10.0.0.5 라는 사설 ip를 사용하고 있을때 overlay network로 전혀 다른 사설대역 192.168.0.0/24를 만들 수 있다. Docker Swarm 을 구성해놓은 상태에서 새로운 서비스를 생성할 때, 네트워크를 미리 만들어 둔 overlay 네트워크를 지정하면 overlay 네트워크를 사용하는 컨테이너가 각각 올라가고 해당 서브넷 내의 ip를 할당받는다. *이때 주의해야 할 점은 tcp 2377, 7946, udp 7946, 4789 포트가 열려 있어야 한다.* overlay 네트워크 생성하고 테스트해보기.1.network 생성 네트워크를 생성하고 docker network create --driver=overlay --subnet=192.168.0.0/24 mynet 네트워크 생성 확인 1[ec2-user@ip-10-0-0-209 ~]$ docker network ls2NETWORK ID NAME DRIVER SCOPE3f2fd318ecc13 bridge bridge local4c9b6d1d75287 docker_gwbridge bridge local554c54d828043 host host local6f400wz9js3cz ingress overlay swarm7h919kze0tfxw mynet overlay swarm82e51e048bc49 none null local9``` 10mynet 이 생성되었다. &lt;br&gt;&lt;br&gt;1112**2.service 생성**1314`docker service create --name overlaytest --network mynet --replicas 3 tutum/hello-world ` 1516tutum에서 제공하는 helloworld 이미지로 테스트 &lt;br&gt;&lt;br&gt;1718**3.네트워크 테스트** 1920master노드에서 ps 실행 및 ip 확인 [ec2-user@ip-10-0-0-209 ~]$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES3bf7f2ad2f5d tutum/hello-world:latest “/bin/sh -c ‘php-fpm…” 10 seconds ago Up 9 seconds 80/tcp hello.3.8h6n98b0pgqxmhamhlxz0fpnr[ec2-user@ip-10-0-0-209 ~]$ docker exec -it 3bf7f2ad2f5d sh/ # ifconfigeth0 Link encap:Ethernet HWaddr 02:42:C0:A8:00:95 inet addr:192.168.0.149 Bcast:192.168.0.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1450 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) 12worker노드에서 ps 실행 및 ip확인 및 ping [ec2-user@ip-10-0-0-31 ~]$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5eb91ab17651 tutum/hello-world:latest “/bin/sh -c ‘php-fpm…” About a minute ago Up About a minute 80/tcp hello.2.usgapik37uc3qpd7tiirrsi5u[ec2-user@ip-10-0-0-31 ~]$ docker exec -it 5eb91ab17651 sh/ # ifconfigeth0 Link encap:Ethernet HWaddr 02:42:C0:A8:00:97 inet addr:192.168.0.151 Bcast:192.168.0.255 Mask:255.255.255.0 UP BROADCAST RUNNING MULTICAST MTU:1450 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 B) TX bytes:0 (0.0/ # ping 192.168.0.149PING 192.168.0.149 (192.168.0.149): 56 data bytes64 bytes from 192.168.0.149: seq=0 ttl=255 time=0.534 ms64 bytes from 192.168.0.149: seq=1 ttl=255 time=0.528 ms64 bytes from 192.168.0.149: seq=2 ttl=255 time=0.492 ms64 bytes from 192.168.0.149: seq=3 ttl=255 time=0.472 ms64 bytes from 192.168.0.149: seq=4 ttl=255 time=0.497 ms64 bytes from 192.168.0.149: seq=5 ttl=255 time=0.511 ms64 bytes from 192.168.0.149: seq=6 ttl=255 time=0.456 ms64 bytes from 192.168.0.149: seq=7 ttl=255 time=0.517 ms64 bytes from 192.168.0.149: seq=8 ttl=255 time=0.491 ms^C— 192.168.0.149 ping statistics —9 packets transmitted, 9 packets received, 0% packet lossround-trip min/avg/max = 0.456/0.499/0.534 ms/ # 콘테이너 끼리 사설 대역으로 통신이 되는것을 확인할 수 있다. &lt;br&gt;&lt;br&gt; ### 참고자료 * 도커 공식 홈페이지 https://docs.docker.com/network/overlay/ * 도커에서 만든 오버레이 네트워킹 데모 https://www.youtube.com/watch?v=nGSNULpHHZc","link":"/2018/07/27/docker-network-overlay/"},{"title":"[docker] overlay network bandwidth test","text":"앞의 포스트에 이어 overlay network를 테스트해보다가 bandwidth가 궁금해졌다. 아무래도 네트워크 레이어를 하나 더 만드는 것이니 네트워크 성능이 노드들 끼리의 통신보다는 안 좋을 것이라 예상됐다. 찾아보니 예전 docker에서는 많게는 50% 정도까지 성능저하가 있었던 것 같다. 테스트 환경: AWS EC2 t2.medium &lt;-&gt; t2.small 테스트 방법: iperf3 을 사용했다. 테스트 내용 ec2끼리 대역폭 테스트 1[ec2-user@ip-10-0-0-31 ~]$ iperf3 -c 10.0.0.209 -t 302Connecting to host 10.0.0.209, port 52013[ 4] local 10.0.0.31 port 56086 connected to 10.0.0.209 port 52014[ ID] Interval Transfer Bandwidth Retr Cwnd5[ 4] 0.00-1.00 sec 83.7 MBytes 702 Mbits/sec 0 362 KBytes 6[ 4] 1.00-2.00 sec 75.9 MBytes 636 Mbits/sec 0 362 KBytes 7[ 4] 2.00-3.00 sec 87.3 MBytes 732 Mbits/sec 43 659 KBytes 8[ 4] 3.00-4.00 sec 88.1 MBytes 739 Mbits/sec 0 659 KBytes 9[ 4] 4.00-5.00 sec 87.5 MBytes 734 Mbits/sec 60 560 KBytes 10[ 4] 5.00-6.00 sec 93.5 MBytes 784 Mbits/sec 0 560 KBytes 11[ 4] 6.00-7.00 sec 92.5 MBytes 776 Mbits/sec 0 560 KBytes 12[ 4] 7.00-8.00 sec 93.0 MBytes 780 Mbits/sec 0 560 KBytes 13[ 4] 8.00-9.00 sec 89.6 MBytes 752 Mbits/sec 0 560 KBytes 14[ 4] 9.00-10.00 sec 90.3 MBytes 757 Mbits/sec 0 720 KBytes 15[ 4] 10.00-11.00 sec 90.7 MBytes 761 Mbits/sec 0 720 KBytes 16[ 4] 11.00-12.00 sec 91.3 MBytes 766 Mbits/sec 0 720 KBytes 17[ 4] 12.00-13.00 sec 95.1 MBytes 798 Mbits/sec 0 720 KBytes 18[ 4] 13.00-14.00 sec 90.6 MBytes 760 Mbits/sec 0 720 KBytes 19[ 4] 14.00-15.00 sec 91.2 MBytes 765 Mbits/sec 0 720 KBytes 20[ 4] 15.00-16.00 sec 90.0 MBytes 755 Mbits/sec 0 720 KBytes 21[ 4] 16.00-17.00 sec 92.3 MBytes 774 Mbits/sec 0 775 KBytes 22[ 4] 17.00-18.00 sec 94.1 MBytes 789 Mbits/sec 87 624 KBytes 23[ 4] 18.00-19.00 sec 91.2 MBytes 765 Mbits/sec 0 675 KBytes 24[ 4] 19.00-20.00 sec 91.9 MBytes 771 Mbits/sec 0 732 KBytes 25[ 4] 20.00-21.00 sec 91.3 MBytes 766 Mbits/sec 0 732 KBytes 26[ 4] 21.00-22.00 sec 88.5 MBytes 742 Mbits/sec 0 732 KBytes 27[ 4] 22.00-23.00 sec 89.9 MBytes 754 Mbits/sec 0 732 KBytes 28[ 4] 23.00-24.00 sec 85.9 MBytes 720 Mbits/sec 0 732 KBytes 29[ 4] 24.00-25.00 sec 92.6 MBytes 777 Mbits/sec 0 732 KBytes 30[ 4] 25.00-26.00 sec 92.6 MBytes 777 Mbits/sec 0 732 KBytes 31[ 4] 26.00-27.00 sec 90.9 MBytes 762 Mbits/sec 65 585 KBytes 32[ 4] 27.00-28.00 sec 89.0 MBytes 747 Mbits/sec 0 585 KBytes 33[ 4] 28.00-29.00 sec 90.0 MBytes 755 Mbits/sec 0 663 KBytes 34[ 4] 29.00-30.00 sec 93.1 MBytes 781 Mbits/sec 0 663 KBytes 35- - - - - - - - - - - - - - - - - - - - - - - - -36[ ID] Interval Transfer Bandwidth Retr37[ 4] 0.00-30.00 sec 2.64 GBytes 756 Mbits/sec 255 sender38[ 4] 0.00-30.00 sec 2.64 GBytes 756 Mbits/sec receiver3940iperf Done. 평균 756Mbps 정도의 대역폭 docker에서 overlay 모드로 만들었을 때의 노드끼리 대역폭 테스트 1[ec2-user@ip-10-0-0-31 ~]$ docker exec -it 89c08bbec6d7 sh2# iperf3 -c 192.168.0.155 -t 303Connecting to host 192.168.0.155, port 52014[ 4] local 192.168.0.154 port 47676 connected to 192.168.0.155 port 52015[ ID] Interval Transfer Bandwidth Retr Cwnd6[ 4] 0.00-1.00 sec 92.0 MBytes 771 Mbits/sec 0 1.51 MBytes 7[ 4] 1.00-2.00 sec 85.0 MBytes 713 Mbits/sec 0 1.51 MBytes 8[ 4] 2.00-3.00 sec 86.4 MBytes 725 Mbits/sec 0 1.51 MBytes 9[ 4] 3.00-4.00 sec 85.3 MBytes 716 Mbits/sec 0 1.59 MBytes 10[ 4] 4.00-5.00 sec 84.7 MBytes 710 Mbits/sec 0 1.59 MBytes 11[ 4] 5.00-6.00 sec 87.5 MBytes 734 Mbits/sec 0 1.59 MBytes 12[ 4] 6.00-7.00 sec 87.8 MBytes 737 Mbits/sec 0 1.59 MBytes 13[ 4] 7.00-8.00 sec 87.6 MBytes 735 Mbits/sec 0 1.59 MBytes 14[ 4] 8.00-9.00 sec 85.5 MBytes 717 Mbits/sec 0 1.59 MBytes 15[ 4] 9.00-10.00 sec 79.2 MBytes 665 Mbits/sec 250 631 KBytes 16[ 4] 10.00-11.00 sec 75.5 MBytes 633 Mbits/sec 0 714 KBytes 17[ 4] 11.00-12.00 sec 81.4 MBytes 683 Mbits/sec 0 795 KBytes 18[ 4] 12.00-13.00 sec 86.2 MBytes 723 Mbits/sec 0 871 KBytes 19[ 4] 13.00-14.00 sec 86.2 MBytes 723 Mbits/sec 0 943 KBytes 20[ 4] 14.00-15.00 sec 83.5 MBytes 700 Mbits/sec 0 1008 KBytes 21[ 4] 15.00-16.00 sec 83.9 MBytes 704 Mbits/sec 0 1.04 MBytes 22[ 4] 16.00-17.00 sec 85.6 MBytes 718 Mbits/sec 0 1.10 MBytes 23[ 4] 17.00-18.00 sec 88.8 MBytes 744 Mbits/sec 0 1.16 MBytes 24[ 4] 18.00-19.00 sec 88.1 MBytes 739 Mbits/sec 0 1.21 MBytes 25[ 4] 19.00-20.00 sec 87.9 MBytes 738 Mbits/sec 0 1.26 MBytes 26[ 4] 20.00-21.00 sec 83.3 MBytes 699 Mbits/sec 0 1.31 MBytes 27[ 4] 21.00-22.00 sec 84.5 MBytes 709 Mbits/sec 0 1.35 MBytes 28[ 4] 22.00-23.00 sec 85.1 MBytes 714 Mbits/sec 0 1.39 MBytes 29[ 4] 23.00-24.00 sec 87.8 MBytes 736 Mbits/sec 0 1.44 MBytes 30[ 4] 24.00-25.00 sec 90.0 MBytes 755 Mbits/sec 0 1.48 MBytes 31[ 4] 25.00-26.00 sec 86.1 MBytes 723 Mbits/sec 0 1.51 MBytes 32[ 4] 26.00-27.00 sec 83.2 MBytes 698 Mbits/sec 0 1.51 MBytes 33[ 4] 27.00-28.00 sec 81.5 MBytes 684 Mbits/sec 0 1.51 MBytes 34[ 4] 28.00-29.00 sec 84.9 MBytes 712 Mbits/sec 0 1.51 MBytes 35[ 4] 29.00-30.00 sec 84.1 MBytes 706 Mbits/sec 214 595 KBytes 36- - - - - - - - - - - - - - - - - - - - - - - - -37[ ID] Interval Transfer Bandwidth Retr38[ 4] 0.00-30.00 sec 2.50 GBytes 715 Mbits/sec 464 sender39[ 4] 0.00-30.00 sec 2.50 GBytes 715 Mbits/sec receiver 평균 715Mbps 약 95% 정도의 성능을 보인다. 더 테스트해봐야 알겠지만 이 정도면 진짜 아주 괜찮은 성능이 아닌가 싶다.","link":"/2018/07/27/docker-overlay-bandwidth/"},{"title":"[ActiveDirectory] DC 구성시 허용해야 하는 포트 ","text":"Active Directory 글을 보고 AWS 상에서 AD를 구성하는 것을 테스트해보고 있다. 하지만 이 글은 AWS의 인프라를 기준으로 작성되어 있지 한다. 여기저기 찾아봤으나 aws 상에서 AD를 구성하는 예제가 많지 않다. 사실 쉽게 AWS에서 quickstart로 나와있어서 굳이 구성 예제가 필요하지 않았던 것 같다. 그래도 사정상 테스트를 해야했기에… 일단은 부딪혀보기로 했다. 첫번재 AD를 만드는 데에는 아무 문제가 없었으나 두번째 AD를 만드는 과정에서 문제가 생겼다. example.com 으로 도메인을 만들었는데 두번째 윈도우 서버에서 그 도메인을 찾지를 못한다. nslookup으로 질의해보니 dns가 응답하지 않는다. 뭐가 문제인지 모르겠다. tcp 포트도 다 열려있는데… 그래서 그냥 모든 트래픽을 허용해버렸다. 된다. 모든 트래픽을 열어주는건 찜찜해서 dns가 어떤 포트를 쓰는지 궁금해졌다. 뒤져보니 나랑 비슷한 질문을 한 글을 발견했다. (질문: NSLookup, how to set to TCP from UDP?) 결론: 그냥 udp 53 포트를 열어주면 되는 문제였다… 질문을 열어보면 dns는 주로 udp 53 포트를 사용하고 512 byte가 넘을때만 tcp를 사용한다고 친절하고 내공있는 답변이 적혀있다. 세상은 넓고 고수는 많다. 어쨌든 AWS 상에서 수동으로 AD를 구성하고, 이전하는 것 까지 테스트를 해보고 긴 포스트를 올려볼 예정이다. 물론, 예정일 뿐이다. 위키피디아에 DNS 항목에도 다음과 같이 적혀있다. DNS primarily uses the User Datagram Protocol (UDP) on port number 53 to serve requests.[3] DNS queries consist of a single UDP request from the client followed by a single UDP reply from the server. When the length of the answer exceeds 512 bytes and both client and server support EDNS, larger UDP packets are used. Otherwise, the query is sent again using the Transmission Control Protocol (TCP). TCP is also used for tasks such as zone transfers. Some resolver implementations use TCP for all queries. (지금보니 그 고수의 댓글은 위키피디아에서 그냥 퍼온 것…) 그런데, TCP 포트는 몇번 포트로 통신하는거지? 그걸 알아야 여는 포트를 최소화 할수 있는데…","link":"/2018/08/06/active-directory-port-allow/"},{"title":"[EC2 - Windows Server] AD가 join되어 있을 때 인스턴스 복제하기 ","text":"AWS 상에서 Windows 서버를 복제하는 일은 Linux 서버를 복제하는 것보다 까다롭다. Linux 서버의 경우 ami만 생성하면 되지만, Windows 서버의 경우 ami 생성 후 다시 그 ami를 통해 인스턴스를 실행하면 여러가지 에러를 맞닥트리게 된다. 특히 AD Domain에 접속되어 있는 윈도우즈 서버를 그대로 ami를 떠서 복제한 경우 제대로 인스턴스가 실행되지 않는다. AD에 조인되어 있는 경우 컴퓨터의 sid 등을 통해 도메인의 멤버인지 확인하는데 ami로 복제하게 되면 이런 값들이 중복이 되서 충돌이 생기기 때문이다. 이런 경우 Sysprep 이라는 작업을 통해 시스템에 관한 정보를 초기화 한 후 ami를 생성해야한다. 대략적인 순서는 다음과 같다. 네트워크 설정 초기화 (ip, dns 모두 자동) sysprep ami 생성 ami 복제 (선택사항) 해당 ami로 인스턴스 실행 AD join 여기서 주의해야할 것은 sysprep 과정에서 시스템이 초기화된다는 것이다. sysprep은 시스템에 관련된 정보들을 초기화한다. 예를들어 sid, 도메인 정보 등이 초기화된다. 따라서 무작정 sysprep을 돌렸다간 해당 인스턴스를 복구 불가능한 상태로 만들수도 있다.","link":"/2018/08/10/copy-ad-joined-instance/"},{"title":"[EC2 - T3] T3 인스턴스 출시!!!!!!!","text":"대박사건. EC2 인스턴스 타입 중 작은타입을 지원하고, 테스트용, 혹은 작은규모의 로직을 돌리기에 적합하던 t타입의 인스턴스 중 3세대가 나왔다. 이전 세대인 t2 타입은 2014년에 처음 나왔고 2016년에 다양한 사이즈들이 추가됐다. 그러니깐, 4년만에 t타입의 새로운 세대가 나온 것. 당연히 t2 타입보다 성능은 올라가고, 가격은 떨어졌다. 버지니아 기준으로 가격은 10%정도 저렴하다. 아쉽게도 서울리전은 아직 지원이 안됨. **사실 cpu 성능보다 더 기대가 되는것은 네트워크 성능.** m4에서 m5로 업그레이드가 됐을 때 cpu 성능도 증가했지만, 기본 네트워크성능이 많이 향상되었다.(관련글: [EC2 인스턴스의 타입별 대역폭을 살펴보자](https://blog.wisen.co.kr/?p=7751)) 마찬가지로 t2에서 t3로 업그레이드 되면서 기본 네트워크 성능이 향상될 것으로 보인다. (low to moderate -&gt; up to 5 Gigabit) 이전 t2 인스턴스의 한계라면 cpu 성능이나 불안정성도 한 몫했지만 단순 웹서버로 사용하기에 아쉬웠던 점은 아무래도 네트워크 성능. 작은규모의 웹사이트의 경우 m5.large 정도의 스펙이 불필요하지만 네트워크 성능 때문에 어쩔 수 없이 m5를 선택하기도 했다. 만약 t3에서 네트워크 성능이 일정 기준치 이상으로 향상되었다면 t3 타입은 꽤 매력적인 인스턴스 타입이 될 것 같다. 정확한 사양은 더 테스트해봐야 알겠지만, 어쨌든 좋은소식임에는 분명하다. 참고: https://aws.amazon.com/ko/blogs/aws/new-t3-instances-burstable-cost-effective-performance/","link":"/2018/08/22/t3-instance-launch/"},{"title":"새로운 테마 적용 - jekyll-swiss","text":"새로운 테마를 적용했다. 원래의 계획은 minima 테마를 커스터마이징 하는 것이었지만, 디자인에는 재능이 없고 커스터마이징이 너무 귀찮고 만들어진 테마 중에 딱 필요한게 구현되어있고 디자인도 마음에 드는걸 발견해서 그냥 테마를 적용하기로 했다. 역시 세상은 넓고 이미 만들어 놓은 좋은 것도 많다. 공부하는 차원에서 만들어보려고 했는데, 굳이 jekyll을 깊게 파면서까지 이걸 커스터마이징할 이유가 없었다. 그래도 꾸역꾸역 기능하나정도 추가하는건 어렵지 않을것 같아서, 다른 jekyll 활용 예를 참고하려고 했고 참고하려고 하니 마음에 드는 테마 하나를 발견했다. 이번에 적용한 테마는 Swiss 이 테마를 적용한 이유는 간단하다. 예쁘다 필요한 기능(카테고리별로 묶어주는 기능)이 깔끔하게 구현되어 있다. 그 외에 잡다한 기능들이 없다. 한 마디로 minima 테마에서 카테고리 별로 정리해주는 기능만 딱 하나 추가되고, 디자인이 예쁘게 정리된 내가 찾던 바로 그 테마인 것이다. 그리고 무엇보다도 다른 기능이 없는 게 중요했다. 다른 다양한 기능이 있다고 해도 내가 쓸 생각이 없기 때문에 기능은 최소의 기능만 있는게 좋다. 그래야 관리도 되고 나중에 커스터마이징 할 때도 코드를 보기가 편하다. 블로그 테마가 계속 마음에 걸렸는데 이걸로 일단 한 동안은 유지할 수 있을 것 같다. 테마 적용방법은 지킬 홈페이지 와 스위스 테마 깃헙 참고. 적용이 매우 간단하다.","link":"/2018/08/29/new-theme-jekyll-swiss/"},{"title":"reviewrepublic 심폐소생술 - (1)프로젝트 수락","text":"서론페이스북에 리뷰왕 김리뷰라는 페이지가 있다. 갖가지 리뷰들이 올라오고 요새 올라오는 카드뉴스식의 리뷰를 선도한(…?) 페이지이다. 그 김리뷰가 리뷰리퍼블릭이라는 서비스를 만들었다. 사용자들이 리뷰를 작성해서 올리고 양질의 리뷰를 올릴수록 그 보상이 작성자에게 돌아가게끔 하겠다는 취지로 만들었다고 한다. 어쨋든 겉으로 보이는 형태는 조그만한 커뮤니티 앱이다. 어떻게 보면 sns같기도 하고. 어쨋든 그런 조그마한 커뮤니티 앱이 월 100만원 가량의 유지비가 나온다는 소식을 들었다. 서비스 유지비용이 월 100만원씩 나온다면 도저히 수지타산이 안나온다. 곧 망할 것 같았다. 곧 망했다. 그래도 서비스는 사비로 유지되고 있었다. 그래서 비용을 줄이는 데에 도움이 되고 싶었다. 마침 클라우드 서비스를 공부하고 있기도 했고 공부하다가 실질적인 서비스를 운용해봐야 좀 더 실질적인 공부가 될 것 같기도 해서 덜컥 김리뷰계정에 메세지를 보냈다. “아무리 봐도 100만원은 오바다. 회사가 망하기도 했고 여러가지로 힘든상황인것 같지만 리뷰리퍼블릭의 취지 자체는 좋은것 같다. 나에게도 한 사이트의 비용구조 개선은 공부가 된다. 도와주겠다.” 라고 연락을 했다. 그러나 첫번째에는 답이 없었다. 다시 연락했고, 답이 왔다. 만나서 자세히 얘기해보기로 했다. 만나서 계정정보들을 받고 살펴보니 왜 100만원이라는 돈이 나오는지는 알 것 같았다. 일단 이미지 서버비용(imgix)이 너무 많이 들고, 안쓰는 리소스들이 너무 많았다. 개발서버, 스테이징 서버들을 클라우드에 올려놓았지만… 개발자가 없는걸? 놀고 있는 서버들이 많았다. 일단 놀고있는 서버들만 정리하면 비용은 좀 줄일 수 있을 것 같았다. 그래서 흔쾌히 작업을 수락했고 금방 끝낼 수 있을것 같았다. 착각이었다. 생각보다 복잡한 구조를 가진 어플리케이션이었고 많이 쓰이지 않는 프레임워크(meteor.js)로 제작되어있었다. 해당 프레임워크를 공부해야했고, 리소스들은 정리가 안되어 있었다. 오버스펙으로 인프라가 구성된 이유가 있었다. 어플리케이션 자체가 비효율적으로 제작되어 있었다. 그러니깐 애초의 생각처럼 인프라 구성만 바꾼다고 되는게 아니라 어플리케이션 자체를 수정해야 비용구조를 개선하고 퍼포먼스도 올릴 수 있는 프로젝트였던 것이다. 결국엔 소스코드까지 뒤져봐야했다. 누구나 처음엔 그럴듯한 계획을 갖고있다.일단 인프라부터 옮기자 그래도 하는 수 없이 일단 인프라 레벨에서부터 접근했다. 메인 어플리케이션이 돌고 있는 서버는 digital ocean이라는 클라우드 서비스를 통해서 운영되고 있었다. 디지털오션의 서비스 자체에는 큰 문제가 없었다. 어차피 복잡한 구조의 어플리케이션이 아니라 단순한 웹어플리케이션이기 때문에 어떤 서버를 이용하든지 큰 상관은 없다. 다만 문제는 서비스 규모에 비해서 지나치게 많은 리소스들이 사용되고 있었다. 아래 그림에서 보이겠지만 실제 서비스에 이용되는것은 로드밸런서 노릇을 하는 서버 한대와 그 밑에 웹서버가 동작하고있는 서버 4대, 즉 5대가 전부였지만, 배치서버, 검색서버, 스테이징 서버, 젠킨스 서버, 개발 서버 등 합쳐서 16개의 서버가 돌아가고 있었다. 돈이 많다면야 이렇게 해놓는게 관리나 개발차원에서 편하겠지만 이미 망해버린 상황에서는 많이 오바였다. 그래서 하나하나 서버들, 서버위에 올라간 코드들을 훑어보면서 실제 서비스와 연관이 없는 서버들은 일단 모두 정지시켰다. 또 다른 문제점은 DB 서버였다. 리뷰리퍼블릭은 몽고DB를 사용하고 있는데 Mlab에서 제공하는 SaaS 형태의 서비스를 이용하고 있었다. 일단은 이 서비스가 매우 창렬하다는게 문제였다. 한달에 $180나 되는 비용을 지불하는데 거기에 돌아가는 인스턴스는 매우 낮은 스펙이었다. 또한 이 서비스를 이용하면 DB서버는 AWS 위에 올라가게 되는데 그렇게 되면 Digital Ocean &lt;-&gt; AWS 간의 구간에서 퍼블릭 인터넷망을 이용해야 했다. 보안상으로도, 네트워크적인 측면에서도 마이너스이다. 결국 DB서버도 마이그레이션을 해야했고 그러던 와중 mongodb에서 직접 운영하는 SaaS형태의 서비스인 Atlas를 검토하게 되었다. 제공하는 기능도 Atlas가 더 많았고 결정적으로 같은 가격에 제공되는 인스턴스의 스펙이 Atlas가 훨씬 좋았다. 게다가 $100 무료쿠폰도 얻었다. 사실 MongoDB를 그냥 AWS 서버 위에 올려서 운영하는 것도 검토해봤지만 Atlas에서 제공하는 on-line 마이그레이션, 성능 모니터링, 자동 fail-over 등의 기능이 마음에 들었다. 내가 직접 관리하게 되면 저런것들을 다 수동으로 작업해야되는데 기본으로 제공해주니 엄청난 메리트였다. DB 서버를 마이그레이션 하기로 결정을 하고 나니 다시 어플리케이션서버가 Digital Ocean에 올라가있는 게 걸렸다. 사실 애초에 AWS로 옮길 생각이었다. 내가 편한 환경이기도 하고 Digital Ocean이 서울에 서버가 없기도 하고, 여타 기능에선 AWS가 압도적이기 때문. 또한 DB가 AWS에 올라가기 때문에 어플리케이션 서버도 AWS에 올라가있는 게 성능면에서나 안정성면에서나 훨씬 낫다. DB 서버가 위치하는 네트워크(VPC)와 어플리케이션 서버가 위치하는 네트워크 사이에 VPC Peering을 맺으면 AWS의 내부망을 이용할 수 있다. 그래서 일단 기본적인 마이그레이션의 가닥은 다음과 같았다. 어플리케이션: Digital Ocean -&gt; AWS DB: Mlab -&gt; Atlas 이러면 일단 인프라적으로는 비효율적인 부분을 많이 개선하는 것이라고 생각했다. (그럴리가…) 다음 포스팅에서는 Digital Ocean -&gt; AWS 로 옮기는 작업때 있었던 일들에 대해서 다뤄보겠다. 리뷰리퍼블릭의 기존 아키텍쳐 리뷰리퍼블릭 변경 아키텍쳐","link":"/2018/09/11/reviewrepublic-log-1/"},{"title":"reviewrepublic 심폐소생술 - (2)DigitalOcean -> AWS","text":"이번 편에선 DigitalOcean 에서 AWS로 서버들을 옮기는 과정을 적어본다. 전에도 말했지만 리뷰리퍼블릭의 어플리케이션은 meteor.js 라는 프레임워크 (조금 더 정확하게는 meteor.js + react.js) 위에서 돌아간다. 이 프레임워크 덕분에 마이그레이션이 두배는 더 어려워졌다. 일단 익숙한 방식 (어디 한 express.js 나 스프링 MVC 정도 사용할 줄 알았는데…)이 아니기에 이 프레임워크의 전체적인 개념부터 어떤식으로 배포가 되는지를 파악해야 했다. 서버 배포방식 확인, 배포 그렇다고 지금 이 포스팅에서 meteor.js의 작동방식에 대해 다룰 것은 아니다. 왜냐하면 다행히 리뷰리퍼블릭의 프라이빗 코드저장소에는 간략한(무려 네줄) 서버 실행 및 배포 방법이 적혀있었기 때문이다. (이렇게 문서화가 중요하다.) 서버 실행은 meteor.js 에 setting.js 파일을 참조해서 하면 되고 배포는 mup.js 라는 배포툴을 이용하고 있었다. 그러니 나는 이걸 aws 인프라 위에서 작동이 되는지 확인만 하면 됐다. 먼저 로컬환경에서 테스트를 해봤다. 내 로컬 컴퓨터에서 정상적으로 어플리케이션이 돌아가는지, 돌아가는 어플리케이션은 실 서비스의 동작과 같은지 확인해야했다. 다행히 하라는 대로 하니 서버가 돌아가긴 했다. 다만 환경변수 세팅이 제대로 안되어있어서 db 접속이 안됐다. 환경변수만 조금 세팅해주니 정상적으로 어플리케이션이 돌아갔다. 또 다행히도 실서비스와 같은 동작을 하는 서버 같았다. (아쉽게도 테스트코드 같은것은 없었다.) AWS에서 올려보려고 했다. 로컬에서와 똑같은 방식으로 실행을 시키려고 하는데 알수없는 이유로 자꾸 meteor가 빌드 중에 죽었다. 정확한 이유를 파악하기 어려웠다. 결국 mup.js를 통해서 다시 시도했다. mup.js 는 해당 meteor 어플리케이션의 도커 이미지를 만들고 그 도커이미지를 통해 원격 서버에서 컨테이너로 실행시켜주는 배포 툴인듯 했다. 몇번의 시도 끝에 EC2 인스턴스 위에 배포하는 데에 성공했다. mup.js에서 미리 세팅해놓은 덕에 환경변수도 깔끔하게 들어가서 db 접속문제도 없었다. 잘 돌아가기에 해당 서버의 AMI를 생성해서 이미지를 저장해놨다. 이렇게 서버 배포는 간단하게 끝이났다. 결국 서버 이전의 방식은 뭐 서버의 이미지를 떠서 새로운 서버를 올리는 방식이 아니라, 그냥 새로운 서버 하나를 생성하고 그 위에 소스코드를 배포하는 방식이 되었다. 그러니깐, Digital Ocean이랑은 아무 상관이 없었다. 그냥 리눅스 가상머신 위에 소스를 배포하기만 하면 되는 일이었다. 다행히 코드저장 및 최소한의 인스트럭션이 남아있었고 이를 토대로 재배포를 정상적으로 수행할 수 있었다. 와… 만약에 그런 최소한의 코드 정리도 안되어 있고 문서화가 안되어있는 상황이었으면… 아마 그냥 포기하지 않았을까. 로드밸런서 변경, 오토스케일링 적용. 일단 기존 로드밸런서에 가용성 문제가 있었다. 기존에는 그냥 인스턴스를 하나 띄워서 그 인스턴스가 로드밸런서의 역할을 한다. 그러다 만약 해당 인스턴스가 뻗어버리면? 바로 서비스 장애로 이어진다. 그러니 인스턴스가 뻗지 않게 사양도 높게 잡아줘야 하고 인스턴스의 가용성 관리도 해야하는건데 그게 쉽지 않다. AWS의 로드밸런서 서비스인 ELB는 고맙게도 그런걸 다 해준다(정확히는 AWS가 보장…해준다고 한다.). ELB 로드밸런서가 뻗지 않게 가용성을 보장해주는 관리형 서비스로 트래픽이 몰려도 이에 맞게 로드밸런서가 대응한다. 아무래도 무식하게 좋은서버 한대 올려서 로드밸런서 구성하는것 보다는 구성도 간단하고 요금도 무작정 성능 좋은 서버 하나에 로드밸런서 올려놓는 것보다 저렴하다. 그래서 로드밸런서를 ELB로 교체했다. 그리고 오토스케일링 적용. 트래픽이 늘어났을때 LB만 살아있다고 되는게 아니라 결정적으로 앱 서버가 살아있어야 한다. 은행에서 티켓 발행하는 기계가 아무리 살아있어도 은행원들이 탈진해버리면 은행 서비스가 불가능한 것과 똑같다. 그러니 앱 서버에도 가용성관리가 들어가야 한다. digital ocean은 아쉽게도 이러한 오토스케일링을 적용하려면 haproxy 등을 통해 직접 구성해야한다. AWS는 이 autoscaling 구성이 간편한게 자랑이다. 그래서 autoscaling을 구성했다. 이렇게 함으로써 얻는 이점은 일단은 가용성이다. autoscaling을 통해 인스턴스가 알수없는 이유로 죽더라도 다시 생성된다. 또한 로드밸런서의 가용성도 올라갔다. 구성은 그냥 아까 서버 배포하면서 생성한 ami를 기준으로 시작구성 생성하고 오토스케일링 적용시켜주면 끝. 보너스… 싱가폴 -&gt; 서울 다른 얘길 적다보니 얘기 안했는데 digitalocean에서 제일 가까운 서버가 싱가폴에 있다. AWS는 서울리전이 지원된다. 응답속도는 적게는 0.2초, 많게는 거의 0.7초정도 차이난다(물론 테스트 환경마다 다름). 그러니 싱가폴에서 서울로 옮기기만 해도 최소 0.2초 정도는 버는 셈이었다. 근데 그럼 뭐해 사이트 로딩하는데 7~8초 걸리는데… 0.2초 차이 티도 안남. 이 글을 적던 도중 리뷰리퍼블릭이 터져버렸다…","link":"/2018/09/18/reviewrepublic-log-2/"},{"title":"AWS RDS mysql - 다른 리전에 읽기복제본 생성: 버전이슈","text":"다른 리전에 읽기복제본을 생성해야 될 때가 있다. 대충 추려보면 다음과 같은 경우. 다른 리전에 데이터를 읽기만 하는 워크로드가 있는 경우 다른 리전에 DR 용으로 데이터를 보관하는 경우 그럴때 RDS의 읽기복제본 생성 기능은 매우 간편하고 효과적이다. 파라미터 그룹을 만진다든지 하는 설정을 해줄 필요 없이 대상 리전에 서브넷 그룹만 잘 설정 되어 있으면 콘솔에서 클릭 몇번으로 복제본을 생성할 수 있다. 읽기복제본 생성방법 생성할 지역 RDS에서 DB subnetgroup 생성 원본 DB에서 Create read replica 클릭 후 설정. 이렇게 해서 읽기 복제본은 한 인스턴스당 5개 까지 만들 수 있다. 그리고 만든 읽기 복제본을 multi-az 구성해 가용성을 높일수도 있다. 꽤 괜찮은 기능인듯? 그런데, 이게 잘 안될 수도 있다테스트를 해봤다. 현재(2018년 9월 28일)기준 서울리전에서 mysql 5.7의 최신 버전은 5.7.23이다. 이 버전으로 mysql을 생성하고 도쿄리전에 read replica를 생성해 보겠다. 안된다…! 그럼 다른 리전에는 될까? 싱가폴엔 된다. 버지니아 리전엔 안 된다. (다른 리전엔 거의 되는 듯 했다.) 뭐지 이게…? 도쿄리전과 버지니아 리전에 장애가 있나? 대체 왜그럴까?일단 에러를 추적해 보자. 에러는 다음과 같다. Cannot find version 5.7.23 for mysql (Service: AmazonRDS; Status Code: 400; Error Code: InvalidParameterCombination; Request ID: ed9ba771-ee3e-4334-b868-e3b3082a4286) 그니깐 5.7.23 버전을 못찾겠다는 말이다. 대체 무슨 문제일까 찾아보니, 도쿄리전, 버지니아 리전 등에는 mysql 5.7.23이 없다. 마찬가지로 mysql 5.6.41버전도 tokyo 리전엔 없어서 읽기 복제본 생성이 불가능 하다. 즉, 읽기 복제본을 만드려고 하는 mysql 버전이 해당 리전에 존재하지 않기 때문에 복제본을 생성할 수 가 없는 것이다. 아니 대체 왜? 보통 도쿄, 버지니아는 업데이트가 가장 빠른 지역이 아닌가. 그나마 의심해 볼수 있는건 최신버전을 RDS에 적용하는데 문제가 있다거나… 홀수 버전은 취급 안한다든가… 등을 의심해볼 수 있겠지만 무슨 이슈가 있는지 정확하게 파악기는 어렵다. 그냥 지금 상태로는 안되는 것이다. 따라서 마이너 버전 업데이트를 롤백하든지… 도쿄리전 등에 마이너 버전이 업데이트 되길 기다리든지 해야 한다. 여튼 RDS에서 다른리전에 읽기복제본 생성시 주의사항 하나가 추가되었다. 대상 리전에 해당 mysql 버전이 있는지 확인할 것","link":"/2018/09/28/cross-region-read-replica-version-issue/"},{"title":"AWS Lightsail VS EC2","text":"Lightsail 이라는 서비스가 나온지는 좀 되었다. 간단하게 소개하면 아마존에서 제공하는 웹 호스팅 서비스 이다. 기본적인 컨셉 자체는 AWS라는 복잡하고 어려운 클라우드 어쩌구를 몰라도 웹호스팅을 다른 업체만큼 싸게 제공해주는 데 목적이 있는 듯하다. 영세한 웹호스팅업계에 AWS라는 공룡이 침입한 게 아닌가 싶다. Digital Oceans 같은 호스팅업체와 제공하는 서비스와 심지어 UI까지 비슷하다. Digital Oceans는 점점 클라우드 업체처럼 되어가고 있고 아마존은 호스팅 업계의 영역까지 군침을 흘리고 있는 모양새다. 어쨌든 Lightsail은 그저 ‘아마존의 웹호스팅 서비스’라는 것이 중요하다. 그런데, AWS를 사용하고 있는 사람 입장에서는 그럼 AWS에서 EC2를 사용하는것과 Lightsail에서 사용하는 것과 뭐가 다른지가 궁금하다. 간단하게 그 차이점을 정리해보려고 한다. Lightsail VS EC2어차피 요금과 기능이 제일 중요하다. 두 가지만 살펴보자. 요금 기능 요금왠만한 조건에서는 Lightsail이 더 싸다. Lightsail은 2GB 메모리, 즉 T2.small 기준으로 $10의 요금을 받는다. 여기에는 60GB의 SSD EBS 볼륨도 포함되어 있고 심지어, 트래픽 요금까지 포함되어있다. EC2에서 t2.small을 3년 약정으로 잡고 (no-upfront), 60gb EBS를 잡으면 $12.62 가 나온다. 물론 여기에 트래픽비용은 별도다. 그러니 한달 내내 사용할거면 무조건 Lightsail이 싸다. (한달 내내 사용하는거니 스팟은 고려하지 않는다.) 그러나, 한달 내내 사용하는게 아니라면? 그럼 얘기가 달라진다. 일단, Lightsail은 월별 과금이다. 즉, 한번키면 ec2 처럼 stop-start가 안된다. ec2에서 자랑하는 초당 과금 뭐 그런거 없다. 그냥 월에 $10씩 꼬박꼬박 내면된다. 즉, ec2를 딱 필요한 시간에만 쓴다면 ec2가 훨씬 적게 나올 가능성이 있다는 말이다. 관리를 잘해주면 ec2는 사용한 만큼만 과금이 되니 굉장히 적은 비용으로 운용이 가능하다. 그러나 매번 껐다 키기는 귀찮다. 이거 잘 지키는 팀 잘 못봤다. (스크립트로 짜놓지 않는이상. ) 기능먼저 EC2에선 되지만 Lightsail에서 안되는 기능들은 대략 다음과 같다. vpc관련 기능이 제한적 인스턴스 타입 변경 정기적인 스냅샷 생성 디테일한 보안그룹설정 IAM role 부여 여러 로드밸런서 옵션 (예: NLB) 등 뭔가 디테일하고 딥한 옵션은 제공하지 않는다. 대신 일반적인 웹호스팅에서 제공하는 기능들은 대부분 제공한다. 또한 Lightsail에서 특별히 제공해주는 기능도 있다. 웹 SSH 콘솔 VPC Peering 웹 콘솔은 디지털 오션같은데에서 기본으로 제공해주는 서비스라 AWS에서도 그냥 제공해주는 것 같다. EC2에서 비슷한 기능을 이용하려면 꽤나 복잡한 과정을 거쳐야한다. 거기에 내 생각에 매력인 옵션은 바로 ‘VPC peering’ 기능이다. Lightsail에서 인스턴스를 생성하면 그게 어떤 VPC에 생기는지 알수 없다. 그렇지만, Lightsail에서는 내 VPC와 Peering할수 있는 기능을 제공한다. 이 기능 덕분에 Lightsail의 활용성이 엄청 높아지는 것 같다. 사설 및 정리사실 이번에 개인적으로 Jira를 사용하려고 작은 EC2를 하나 띄워놨다. 근데 이게 아무리 생각해도 비용적으로 비효율적인게 t2.medium을 띄워놨는데 이게 한달에 거의 4만원 돈이다. 이럴 거면 그냥 jira cloud를 이용하는게 낫다. 이것보다 더 저렴한 옵션은 없나 찾아보던 중에 Lightsail을 발견했다. 구축한 Jira를 옮기는게 귀찮아서 지금 당장 Lightsail을 쓰진 않겠지만 다른 프로젝트를 할때는 충분히 고려할 만한 옵션인 듯 하다. 단독으로 웹서버를 띄우는 등의 간단하고 작은 규모의 서비스에게 Lightsail은 굉장히 매력적인 옵션으로 보인다. 가격이 거의 절반 가격이고, 매달 일정한 금액이 나오기 때문에 비용관리 측면에서도 유리하다. 트래픽도 기본 1TB씩 제공하기 때문에 크게 부족할 일이 없을 것 같다. DB 등 중요한 인프라는 기존 AWS 서비스를 사용해서 관리하고 이에 대한 연결은 Lightsail과 VPC Peering을 이용해서 가능하다. 뭔가 엄청난 트래픽이 예상되는 서비스만 아니면 왠만한 서비스에서 모두 고려할 만 하다. 물론 안정적인 서비스를 원하는 팀에게는 추천하기 애매하다. 일단 SLA를 제공하지 않고, t2타입의 특성상 장애가 발생할 가능성이 있다. 또한 VPC 관리 등이 되지 않기 때문에 뭔가 불투명하고 막연하다. 웹호스팅에 익숙한 업체들이야 상관 없겠지만, 아닌 경우 뭔가 보안에 취약해보이고, 허접한 서비스 같아보인다. $10~$20 가 중요하지 않은 업체는 그냥, m5.large를 multi-az 구성해서 튼튼하게 가는게 여러가지로 낫다. 즉, 만원 이만원이 중요하고, 매달 일정한(그리고 아주 작은) 요금이 나오길 바라는 팀, 예를들어 스타트업은 Lightsail을 적극 고려하시길. 또한 개인 웹사이트 등을 고려하는데 뭔가 AWS를 사용해보고 싶다면, 사용해볼만하다. 다른 호스팅업체보다는 약간 비싸지만 무지막지하게 비싼 가격은 아니라고 생각하고, 다른 클라우드에 비하면 싸다. 끝.","link":"/2019/03/21/lightsail-vs-ec2/"},{"title":"AWS Athena, CF Log 분석을 위한 쿼리 예시","text":"AWS Athena, Cloudfront Log 분석을 위한 쿼리 예시Cloudfront는 고맙게도 Edge단에서 발생하는 처리 로그들을 모아서 제공해 줍니다. CF Log는 기본적으로 S3에 gz 형태로 압축되엇 제공이 되고다음과 같은 DDL을 통해 테이블을 정의하면 Athena에서 조회할 수 있습니다. 1CREATE EXTERNAL TABLE IF NOT EXISTS default.cloudfront_logs (2 `date` DATE,3 time STRING,4 location STRING,5 bytes BIGINT,6 request_ip STRING,7 method STRING,8 host STRING,9 uri STRING,10 status INT,11 referrer STRING,12 user_agent STRING,13 query_string STRING,14 cookie STRING,15 result_type STRING,16 request_id STRING,17 host_header STRING,18 request_protocol STRING,19 request_bytes BIGINT,20 time_taken FLOAT,21 xforwarded_for STRING,22 ssl_protocol STRING,23 ssl_cipher STRING,24 response_result_type STRING,25 http_version STRING,26 fle_status STRING,27 fle_encrypted_fields INT28)29ROW FORMAT DELIMITED 30FIELDS TERMINATED BY '\\t'31LOCATION 's3://CloudFront_bucket_name/AWSLogs/ACCOUNT_ID/'32TBLPROPERTIES ( 'skip.header.line.count'='2' )33``` 34출처: https://docs.aws.amazon.com/ko_kr/athena/latest/ug/cloudfront-logs.html3536위의 쿼리문에서 다른것은 수정할 것 없이 `LOCATION` 뒤의 s3 버킷 주소만 자신의 버킷에 맞게 수정해주면 됩니다. 3738이렇게 테이블 정의를 하고 나면 필요에 따라 쿼리를 날려 CF로그를 분석 할 수 있습니다. CF Cache Statistics 등에서도 통계를 제공해주기 때문에 통계에 관한 쿼리가 별로 필요하지 않을 수도 있지만, 따로 대시보드를 구성해 시각화 툴과 연계한다던가 할때는 필요할만한 것들을 모아봤습니다. Troubleshooting이 필요할 때는 그때그때 상황에 맞게 쿼리를 해야해서 예시를 들기가 어렵네요. 아래 예시 쿼리들은 참고만 하시고 필요에 맞게 변경해서 사용하시기 바랍니다. 3940# 쿼리 예시 4142## 일별 사용량 (GB)43```SQL44SELECT 45 date, sum(bytes)/1024/1024/1024 as GB 46FROM 47 \"sample_db\".\"cloudfront_logs\" 48GROUP BY49 date; CloudFront 응답 별 카운트1SELECT 2 response_result_type, count(*) as cnt 3FROM 4 \"sample_db\".\"cloudfront_logs\" 5GROUP BY 6 response_result_type7ORDER BY 8 cnt DESC; 에러의 응답 코드 별 조회1SELECT 2 status, count(*) as cnt3FROM 4 \"sample_db\".\"cloudfront_logs\"5WHERE 6 result_type = 'Error'7GROUP BY 8 status9ORDER BY 10 cnt DESC; 상위 20개 에러(4XX,5XX) uri1SELECT 2 uri, count(*) as cnt 3FROM 4 \"sample_db\".\"cloudfront_logs\"5WHERE 6 result_type = 'Error'7GROUP BY 8 uri9ORDER BY 10 cnt DESC11limit 20; 상위 20개 요청ip count1SELECT 2 request_ip, COUNT(*) AS cnt 3FROM 4 cloudfront_logs5GROUP BY 6 request_ip7ORDER BY 8 cnt DESC; 상위 20 Referrer Count1SELECT 2 referrer, count(*) as cnt 3FROM 4 \"sample_db\".\"cloudfront_logs\"5GROUP BY 6 referrer7ORDER BY 8 cnt DESC 9LIMIT 20; 시계열 분석 예시시계열 분석을 위해서는 기본 테이블에 저장된 date, time의 데이터를 concat(), from_iso8601_timestamp(), date_trunc() 등을 이용해 변환해주어야 합니다.Athena는 Select 절에서 Alias가 잘 만들어지지 않아서 cte를 쓰는 편이 편합니다. CTE 예시1WITH cte AS (2 SELECT from_iso8601_timestamp(concat(concat(date_format(date, '%Y-%m-%d'), 'T'), time)) AS ts3 FROM \"sample_db\".\"cloudfront_logs\"4) 5월 29일(UTC 기준) 시간별 요청수, 오름차순1WITH cte AS (2 SELECT from_iso8601_timestamp(concat(concat(date_format(date, '%Y-%m-%d'), 'T'), time)) AS ts3 FROM \"sample_db\".\"cloudfront_logs\"4)SELECT 5 date_trunc('hour',ts) as TIME, count(*) as CNT6FROM cte7WHERE 8 ts &gt;= from_iso8601_timestamp('2019-05-29T00:00:00') AND 9 ts &lt; from_iso8601_timestamp('2019-05-30T00:00:00')10GROUP BY 11 date_trunc('hour',ts)12ORDER BY 13 date_trunc('hour',ts); 5월 29일(UTC 기준) 시간별 전송량(Gb)WITH cte AS ( SELECT from_iso8601_timestamp(concat(concat(date_format(date, '%Y-%m-%d'), 'T'), time)), bytes AS ts FROM \"sample_db\".\"cloudfront_logs\" )SELECT date_trunc('hour',ts) as TIME, sum(bytes)/1024/1024/1024 as Gb FROM cte WHERE ts &gt;= from_iso8601_timestamp('2019-05-29T00:00:00')AND ts &lt; from_iso8601_timestamp('2019-05-30T00:00:00') GROUP BY date_trunc('hour',ts) ORDER BY date_trunc('hour',ts); 5월 29일(UTC 기준) 시간별 에러수WITH cte AS ( SELECT from_iso8601_timestamp(concat(concat(date_format(date, '%Y-%m-%d'), 'T'), time)) AS ts FROM \"sample_db\".\"cloudfront_logs\" )SELECT date_trunc('hour',ts) as TIME, count(*) as CNT FROM cte WHERE ts &gt;= from_iso8601_timestamp('2019-05-29T00:00:00') AND ts &lt; from_iso8601_timestamp('2019-05-30T00:00:00') AND result_type = 'Error' GROUP BY date_trunc('hour',ts) ORDER BY date_trunc('hour',ts); 시간별 평균 전송시간WITH cte AS ( SELECT from_iso8601_timestamp(concat(concat(date_format(date, '%Y-%m-%d'), 'T'), time)) AS ts, time_taken FROM \"sample_db\".\"cloudfront_logs\" )SELECT date_trunc('hour',ts) as TIME, avg(time_taken) as avg_time FROM cte WHERE ts &gt;= from_iso8601_timestamp('2018-05-29T00:00:00') AND ts &lt; from_iso8601_timestamp('2019-05-30T00:00:00') GROUP BY date_trunc('hour',ts) ORDER BY date_trunc('hour',ts); 주의할 점Athena는 스캔한 데이터의 양에 비례해서 과금이 됩니다. 그런데 Cloudfront Log는 보통 양이 상당히 많은데도 데이터가 파티셔닝되지 않고 하나의 버킷에 통째로 저장이 되죠. 결국 Cloudfront의 Log 양이 상당해지면 Athena가 스캔하는 데이터의 양도 늘어나고 그러면 성능도 느려지겠죠. 따라서 데이터의 양이 많아지는 경우, 파티셔닝을 고려해봐야 합니다. 여기 AWS 블로그에서 Cloudfront로그를 파티셔닝하고, 파싱해서 더 많은 정보를 부여 (예를들어, bot인지 아닌지 여부를 판단해서 isBot이라는 column을 추가)하는 예제를 만들어 놓았네요. 따라해보진 않았지만 꽤 유용할것 같습니다. 글이 포스팅된지 꽤 된것 같은데 한국어로 번역된글도 없는걸 보니 한번 따라해보면서 번역글을 작성해도 괜찮겠네요.","link":"/2019/06/07/athena-cf-log-analysis/"},{"title":"Amazon Elasticsearch Dedicated Master의 스토리지 비용은?","text":"Amazon Elasticsearch Dedicated Master의 스토리지 비용은?Amazon Elasticsearch 에는 Dedicated Master를 설정해줄 수 있습니다. 마스터노드의 기능만 하는 노드를 따로 빼서 클러스터의 안정성을 높이는 방안입니다. Elastic의 문서에도 클러스터의 사이즈가 큰 경우 마스터 노드를 분리하는 것을 권장하고 있네요. (참고: https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html) 오늘 내용은 Dedicated Master에 대한 디테일한 내용이 아니라 비용에 관한 간단한 얘기입니다. 결론부터 말씀드리면 Dedicated Master는 노드당 비용만 받고 스토리지 비용은 과금되지 않습니다. Amazon Elasticsearch를 Production 타입으로 만들면 아래 그림처럼 Data Instances, Dedicated master instances, Storage 등을 선택할 수 있습니다. 그런데 이 Storage 설정 부분은 Data node에만 적용이 됩니다. 여기에서 설정한 Storage 양 X 데이터 노드의 수가 클러스터의 총 용량이 되는거죠. 그럼 Dedicated master instance 에는? Storage 관련해서 설정해주는 부분이 눈씻고 찾아봐도 없습니다.그래서 aws에 support case를 열어 물어보았더니 아래와 같은 답변이 왔습니다. I would like to mention that Storage settings do not apply to any dedicated master nodes in the cluster.Hence, the pricing for dedicated master instances would be based on their instance type and not for the storage. 즉, Dedicated master instance는 그냥 instance 요금만 받는다는 겁니다. 하긴 Dedicated master instance 만해도 기본 인스턴스를 3개씩 써야되는데 스토리지 조금정도는 서비스해줄 수 있겠네요. 별로 중요한 정보는 아니지만, aws 문서에 잘 나와있지 않아서 한번 포스팅해봅니다.","link":"/2019/06/07/amazon-elasticsearch-dedicated-master-free-storage/"},{"title":"Lambda의 동시성에 대하여 - 1","text":"Lambda가 진정한 cloud native인걸까!300원에 200만뷰 소화하기라는 포스팅을 읽은 적이 있습니다. 그리고AWS와 함께 천만 사용자 웹서비스 만들기라는 슬라이드도 본 적이 있습니다. 저 슬라이드를 처음 봤던 때에는 Lambda에 대해서 많이 사용을 안해봐서 그런지 저 자료들만 보면 왠지 엄청난 트래픽을 아주 적은 비용으로 큰 문제 없이 수용할 수 있을 것만 같았습니다. 그러면서 언젠가 full-serverless 로 구성된 서비스를 한번 만들어 봐야겠다는 생각을 했었습니다. 생각만요. 생각해보면 단순히 REST API에 대한 요청만 받아주는 서비스라면 API GATEWAY + Lambda 로만 구성하는게 충분히 메리트 있을것 같았습니다. 그러면 정말 진정한 의미에 Pay-as-you-go 를 구현하는 클라우드 네이티브를 구현해볼 수 있지 않을까요…? Lambda@Edge와의 만남그러던 중 우연찮게 Lambda@Edge를 사용해야 하는 일이 생겼습니다. 특정 헤더를 추가하거나 제거하는 단순한 로직을 Cloudfront에 추가해야 하는데 Cloudfront 자체로는 그런 기능을 제공하지 않기 때문입니다. 개발자체는 간단했습니다. 그런데 막상 실 트래픽에 적용하려고 하니 Lambda@Edge의 동시성 제한이라는 항목이 눈에 들어왔습니다. Lambda@Edge는 각 리전에서 1000번의 동시실행 한도를 갖는다고 나와있습니다. 이게 막상 딱 “1000번 동시실행 제한” 이라는 말만 들으면 ‘뭐야 1000밖에 안돼?’ 라는 생각이 듭니다. 그래서 결국 당시에 서비스 오픈 전에 급하게 Limit 증설 요청을 했던 기억이 납니다. 게다가 저 Limit은 증설을 원하는 Region을 정하고 사유를 말해야 합니다. 결국 그때 막 support 엔지니어랑 전화도 하고 그래서 겨우 Limit을 높여놓은 기억이 있습니다. 그때 사실 결국 트래픽이 많이 들어오진 않아서 저 Limit 내에서 처리가 되긴 했습니다. 그런데 그 당시 트래픽을 아주 일부 (약 1%?) 가량만 태운거라서 만약에 전체 트래픽을 부었으면 무조건 장애가 발생하는 상황이 났을겁니다. 1%만 부었는데도 peak로 발생했던 동시성이 한 300정도는 되었으니까 단순히 10배의 트래픽이 온다고 가정해도 limit 1000은 훌쩍 넘어가겠죠. Limit 1000이 적은걸까 많은걸까?‘Limit 1000’ 이라는 게 사실 작은 수치는 아닙니다. 왜냐하면 이건 단위가 tps같은게 아니라 말 그대로 ‘동시 실행 수’이기 때문이죠. 굳이 풀어서 설명하자면 ‘어느 한 순간에도 Lambda 함수가 동시에 1000개 이상 실행될 수는 없다’ 는 말입니다. 이걸 다시 풀어서 얘기하면, 좀 말도 안되는 가정이긴 합니다만 약 100ms 이내에 끝나는 로직이 완전 동기적으로 1000개가 동시에 실행되고 끝난다고 한다면 이 함수의 tps?는 단순 계산으론 1만 tps 이상이 됩니다. 그렇지만 이게 Limit이기 때문에 또 엄청 넉넉한 수치는 아닙니다. 특히 CDN은 보통 대량 트래픽을 가정하기 때문에 1초에 1만정도 처리한다고 가정해도 어떻게 보면 턱없이 모자란 수치일 수도 있는거죠. 그래서 저는 Lambda@Edge가 참 애매한 서비스라고 생각을 합니다. 물론 모든 로직을 Origin Response, Origin Request 등으로 몰아서 최대한 캐싱이 되게끔 만들면 대규모 트래픽에도 대응이 가능할 것 같긴한데, 그럴거면 그냥 원본서버 건드리는게 더 빠르지 않을까요? CDN 자체의 성능을 훼손하지 않으면서 캐시와 사용자 사이의 logic을 컨트롤 할수 있어야 진정한 Lambda@Edge가 가능한 것 같고 그러려면 Viewer 단의 Lambda 동시성이 CDN이 제공하는 대역폭과 동시성 만큼을 커버해 줘야 할텐데 지금 제공하는 1000으로는 엄청 부족하죠. 즉, viewer 단의 Lambda를 적용해서 뭔가 이득을 보기엔 조금 애매해 보입니다. 물론 이 동시성을 늘려주긴 합니다. 그런데 보니깐 이 Limit을 늘려주는 데에도 꽤 많은 시간이 소요되더라구요. ELB Prewarming 처럼 바로바로 되는건 아닌 것 같았어요. (물론 ELB 프리워밍도 미리미리 해두어야 합니다.) 같은 맥락에서 일반적인 서비스의 was 대신에 이 Lambda를 이용하기에도 좀 애매한 수치가 됩니다. 즉, 저는 이 Limit 1000 이라는 수치가 애매한 수치로 보입니다. 트랜잭션이나 비지니스 로직이 복잡하고 많이 일어나는 어플리케이션에서는 이 수치가 매우 부족할수도 있을것 같네요. 그리고 유저 수가 동접 1만 명이 훌쩍 넘는다면 1000으론 어림도 없을 것 같습니다. 그런데 그냥 동접 1만명이 안되는 서비스 라면, 이 Limit 때문에 Lambda 도입을 망설이지는 않았음 좋겠습니다. 오히려 작은 규모의 서비스에 Lambda를 적용했을 때 비용적인 측면에서 효과가 엄청날거라고 생각하거든요. 갑자기 결론?결국, ‘Lambda가 정말 좋긴한데 이 동시 실행 수 제한이라는 걸 서비스 구축시에 고려해야 한다’는 글이 이렇게 길어 졌네요. 기왕 길어진 김에 사설을 덧붙이자면, 천만 사용자를 가정한 서비스의 모든 백엔드를 Lambda로 구축할 수 는 없을 것 같습니다. 그런데 한 1만? 정도 까지는 충분히 Lambda만으로도 백엔드를 구축할수 있지 않을까 생각해봅니다. S3, API Gateway, Lambda, DynamoDB 만으로 이루어진 1만 사용자 어플리케이션… 한번 만들어 보고 싶네요. 다음 글에선…이번 글은 사실 ‘Lambda의 동시성을 컨트롤 하는 방법’의 서론을 쓰다가 길어져 버린 글입니다. 다음 글에서는 진짜 Lambda의 동시성을 컨트롤 하는 법에 대해서 한번 써볼 생각입니다. 여담…좀 이상한 방향으로 글이 흘러갔다는걸 느끼면서도 오늘은 일단 글을 오랜만에 쓴다는 것과 그 글을 마무리하는 데에 가장 큰 목적이 있기에 이렇게 중구난방한 글을 적습니다. 저는 책을 읽을 때 서론 읽는게 제일 재밌던데 쓸 때도 마찬가지로 서론 쓸 때가 제일 재밌는것 같아요. 기슬 블로그를 쓸 때는 유용하려면 막 자료도 들어가야 되고 검증된 코드도 들어가야되고 하잖아요. 근데 오늘 쓴 서론?처럼 약간 전체 글에대한 소개이면서도 그 주제에 대한 저의 정리이기도 한 글이 쓸 때 재밌는 것 같네요. 무슨 코드 한 줄 없이 기술블로그를 쓰나 싶지만서도 오히려 이런 글들이 있는 블로그도 나름 재미있지 않으려나요. 방법이나 실제 practice는 없고 의견만 있는 이런 글이 실질적인 도움이 되지는 않겠지만, ‘아 이 사람은 이런 기술에 대해서 이렇게 생각하는구나’는 걸 보는 것도 필요하지 않을까 싶네요.","link":"/2019/11/04/lambda-with-sqs/"},{"title":"블로그 개편","text":"블로그 개편했어요!전에는 Jekyll 을 썼었는데 계속 테마는 마음에 안들었었거든요.그래서 예쁜 테마를 찾아보니… 예쁜 테마는 모두 hexo에 있더군요. 그래서 갈아탔습니다. 근데 이거… 너무 좋네요. 지금 적용된 테마는 icarus 라는 테마입니다. 깔끔하고 제공해주는 기능이 딱 제가 필요한거여서 바로 적용했는데.이거 생각보다 엄청난 테마네요… 내용 변경","link":"/2019/12/04/new-blog-hexo-icarus/"},{"title":"문제 해결: Cannot Call methods on a stopped SparkContext","text":"스파크로 이것저것 테스트해보기 위해 zeppelin을 사용하는데 갑자기 이런 에러가 발생했다. java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext. 즉, 지금 사용하려는 SparkContext가 이미 종료되었다는 것이다. 이걸 구글에 검색해도 ‘로그를 봐라’ 라는 식의 원론적인 답변이 나와서 헤매다가 해결방법이 떠올랐는데… 해결 방법그냥 스파크 인터프리터를 재시작하면 된다. 인터프리터를 재시작하는 방법은 다음과 같다. 제플린 우측상단의 유저이름 클릭 -&gt; Interpreter 클릭 인터프리터중 spark를 찾아 restart 버튼 클릭 restart를 누르면 스파크 인터프리터가 재시작이 되고, 조금 뒤에 다시 zeppelin paragraph를 다시 실행시키면 정상적으로 스파크컨텍스트를 받아올 수 있다. 안되면 일단 껐다 켜보자. 데이터가 날아가지만 않는다면. 원인 (추측)사실 문제가 생겼을때 해결 방법보다 그 원인이 더 중요하다. 왜 잘 쓰던 sparkcontext가 사라졌을까…?뭔가 session timeout과 관련이 있을것 같아서 찾아보았는데 아직 명쾌하지 않다.뭔가 제플린의 Interpreter Lifecycle Management과 관련이 있을듯 한데 앞뒤가 안맞는 구석이 있다. 조금 더 확인해 보자…","link":"/2019/12/17/zeppelin-stopped-sparkcontext/"},{"title":"Amazon Linux 2에 Airflow 설치하기","text":"당연히 쉽게 될줄 알았던 airflow 설치에 꽤 애를 먹었다. 구글링을 해봐도 명확한 가이드가 나오질 않아서 내가 성공한 설치 방법을 공유한다. 1# 의존성 설치 (가이드 상에는 python3, gcc, gcc-c++ 정도만 설치하면 된다는데 그렇게만 하면 자꾸 에러가 난다. gcc를 실행을 못한다든지...)2$ sudo yum update -y3$ sudo yum install group \"Development tools\" -y4$ sudo yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel python3-devel.x86_64 cyrus-sasl-devel.x86_64 -y5$ sudo yum install libevent-devel -y67# 드디어 airflow를 설치. 여기에서 sudo를 생략하면 권한문제가 발생한다. 8$ sudo pip3 install apache-airflow910# 위의 과정을 마쳤으면 여기서부터는 공식 가이드 그대로. 11$ airflow initdb12$ nohup airflow webserver -p 8080 &gt; webserver.out &amp; 13$ nohup airflow scheduler &gt; scheduler.out 다른 OS에서는 확인을 못해봤고 Amazon Linux2가 올라간 EC2에서는 정상적으로 작동된다. (2020.1.8 기준)","link":"/2020/01/08/airflow-install-amazon-linux-2/"},{"title":"Bastion Host를 통해 내부 서버에 SSH 접속하기 (터널링)","text":"aws 상에서 보안을 위해 그림과 같이 실제 사용하는 인스턴스는 private subnet에 숨기고, public에 bastion host 라는 걸 둬서 접근하는 방식이 있다. 이렇게 하면 관리할 구멍을 하나만 만들고 그 외에는 접근이 원천적으로 불가능해지기 때문에 보안적으로는 좋아지긴 하는데… 그 내부로 접근하기가 매우 귀찮아지는 단점이 있다. 결국 private subnet 안에 있는 인스턴스에 접근하려면 배스쳔호스트를 거쳐야 하는데,매번 그 명령어를 까먹어서 기록해둔다. 방법은 아래와 같다 1.타겟의 22번 포트를 배스천호스트를 통해 로컬(내컴퓨터)의 33322 포트로 포워딩포트 포워딩 1ssh -i \"bastion-host-key.pem\" -N -L 33322:{target-private-ip}:22 ec2-user@{bastion-host-public-ip} (주의) 새 터미널을 띄운 후 2.로컬호스트의 33322포트를 이용해 타겟으로 접속접속 1ssh -i \"target-key.pem\" -p 33322 {target-user(대체로 ec2-user)}@localhost 타겟의 22번 포트를 배스천호스트를 통해 로컬(내컴퓨터)의 33322 포트로 포워딩 (용어정리) bastion-host-key.pem: bastion host에 접근할 수 있는 key, target-private-ip: 최종 접속할 인스턴스의 프라이빗 ip, target-key.pem: bastion host의 key, bastion-host-public-ip: 말 그대로 bastion host의 퍼블릭 ip 33322포트는 임의로 정한것이며 아무거나 남는 포트를 정하면 된다.끝.","link":"/2019/12/20/bastion-host-port-forwarding/"},{"title":"RDS, DocumentDB 는 Readreplica 생성시 다운타임이 발생하나요?","text":"TL;DR RDS의 경우 single-az에 스냅샷을 한번도 생성한적이 없으면 I/O 지연이 발생할 수 있음 Aurora, Document DB 는 공유스토리지 덕분에 single instance인 경우에도 순단이 발생하지 않음. Readreplica, 읽기 부하 분산RDS, Document DB 등 AWS에서 제공하는 관리형 DB는 Readreplica를 손쉽게 만드는 기능을 제공합니다. ‘읽기복제본’이라는 이름처럼 이렇게 생긴 복제본은 읽기 전용이기 때문에 쓰기 부하분산은 불가능하지만 읽기 부하는 충분히 분산시킬 수 있습니다. 그래서 수평확장이 어려운 DB에 그나마 읽기 성능만이라도 수평확장하기 위해서 많이 사용하죠. 그런데 혹시…얼마 전 이런 질문을 받았습니다. “읽기복제본 생성시에 순단이 발생하나요?” ‘에이 설마…’ 라고 생각했지만, 그래도 확인이 필요했습니다. 급하게 AWS 문서를 뒤져봤고 문서에서도 뭔가 ‘순단따윈 없습니다’ 라는 명백하고 시원한 문구가 안보여 support 엔지니어에게 물어보기로 했습니다. (서포트 케이스 내용) 나: RR 만들때 순단 없어?AWS: 응 없어.(아니 그렇게 단호하게..?)나: 아니… 문서보니깐 RDS는 snapshot 이용해서 readreplica 만든다고 되어 있고, snapshot 만들때 뭔가 하드웨어적으로 부하가 발생할것 같은데 정말 없어…?AWS: 응 없어. 그래서 AWS 서포트 엔지니어가 답변한 내용을 정리하면 readreplica 만들 때 snapshot을 이용하는건 맞음. single AZ로 설정되어 있는 경우 snapshot 생성시에 I/O 중단(suspension)이 발생할 수 있지만 생성된 스냅샷이 있으면 증분식으로 스냅샷이 생성되므로 I/O 중단이 크지 않고, I/O 중단이 DB 중단을 의미하는 건 아님. (아마 read/write latency가 발생하겠죠?) multi-az로 설정되어 있는 경우 마스터가 아닌 stand-by 인스턴스에서 스냅샷을 생성하기 때문에 master에 영향 없음 aurora는 \b공유 스토리지를 사용하는 아키텍처이고 따라서 readreplica 생성시에 스냅샷을 사용하지 않음. 결국 master에 영향 없음. DocumentDB도 공유 스토리지를 사용하므로 마찬가지. 즉, readreplica 생성 시에 마스터 DB에 지연이 생길 수 있는 조건은 Aurora/DocDB 등 공유스토리지를 아닌 RDS를 이중화따위 고려하지 않고 Single-AZ 로 구성되어있어서 stand-by 인스턴스가 없으며 백업따위 한적이 없어서 snapshot이 없다. 위의 세가지 조건을 모두 만족해야 체감이 될만한 중단을 느낄 수 있습니다. 그런데 저 3가지를 모두 만족하는 조건이라면… 정말 초저비용으로 서비스를 만들었으며 의도적으로 백업을 안하도록 설정해야하는건데… 그러지 마요 제발","link":"/2020/04/22/readreplica-downtime/"},{"title":"AWS Athena, Zeppelin과 함께 사용하기 (feat. ALB 로그분석)","text":"Athena, 심플하고 강력하지만 아쉬운 인터페이스이번 포스팅에서는 Athena 와 Zeppelin을 엮어서 사용하는 방법에 대해서 정리하려고 합니다. Athena는 써보면 써볼수록 요물이라는 생각이 드는 제품입니다. S3에 데이터만 잘 저장해 놓으면 그걸 SQL문으로 빠르게 쿼리할 수 있다는 컨셉이 간단하지만 강력하네요. 다른 분산처리 플랫폼도 많지만, 일단 간단하게 시작하기에는 Athena가 최적인것 같아요. 다만, Athena가 워낙 심플한 구조를 갖고 있고 제공해주는 기능도 심플하다 보니 조금 아쉬운점도 있는데요, 일단 제 생각에 가장 아쉬운 점은 인터페이스 입니다. ‘대화형 쿼리’ 라는 컨셉으로 단순한 쿼리창만 갖고 있지만, 사실 ‘대화형’ 이라는 말처럼 채팅기록이 남진 않는게 제 생각엔 커다란 단점인 것 같아요. 우리나라 정서가 워낙 UI에 이것저것 많은 기능을 좋아해서 상대적으로 AWS의 UI가 좀 빈약하다는 느낌을 많이 받는데 Athena의 인터페이스도 그 중 하나인 것 같습니다. 그래도 ‘대화형’이라는 컨셉을 완성하기 위해서는 조금 더 인터페이스가 발전해야 할 것 같다고 생각합니다. 예를들어, 웹로그를 분석하는 과정에서 이전 쿼리의 결과를 들고 있어야 하는 경우가 있습니다. 다음과 같이 요청수 top 10의 정보를 받아왔다고 합시다. -- 요청수 top 10 client ip select client_ip, count(*) as requests from &quot;logs&quot;.&quot;alb_logs_apache&quot; group by client_ip order by requests desc limit 10;그리고 이제, 여기에 나온 client ip 각각에 대해 다음과 같은 쿼리를 통해 어떤 행동을 하는지 알아본다고 합시다. -- top 10 ip 각각에 대해 어떤 행동을 하고 있는지 조사 select * from &quot;logs&quot;.&quot;alb_logs_apache&quot; where client_ip=&apos;000.000.000.000&apos;이런 작업을 하려고 하면 athena에서는 쿼리창을 따로 만들어 놔야 합니다. 사실 쿼리 창을 따로 만들어 놓아도 결과가 잘 저장되지 않는 버그(?) 같은게 있습니다. Zeppelin으로 인터페이스를 보강이러한 단점을 단번에 해결해주는게 바로 Zeppelin 입니다. Zeppelin은 Jupyter 처럼 노트북 환경을 제공해주는 오픈소스 프로젝트입니다. Zeppelin에 대한 소개는 https://medium.com/apache-zeppelin-stories/%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4-%EC%9D%BC%EA%B8%B0-2-apache-zeppelin-%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80-f3a520297938 에 잘 정리되어 있네요. 짧게 설명을 드리면, Zeppelin은 Spark 작업을 편하게 하기 위해서 만들어 놓은 노트북이지만, 써보니 다른 분석 툴과도 같이 쓰이고 Athena 와도 매우 잘 맞는 노트북인 것 같습니다. Zeppelin 화면 예시 위에서 언급했지만 Athena에서 주창하는 ‘대화형 쿼리’의 컨셉을 결국 완성시키려면 ‘노트북 인터페이스’ 가 있어야 완성이 되는것 같아요. 쿼리문의 특성상, 그때 그때 궁금한 것을 쿼리로 날리려면 이전의 결과를 들고 있어야 하는 경우가 매우 많고, 또한 그 쿼리문의 결과를 시각화해서 볼 수 있으면 사용성이 정말 매우매우 좋아집니다. 만약 제가 아마존의 Product 팀에 있다면 Athena의 웹 인터페이스를 당장 노트북 인터페이스로 바꾸자고 하고 싶을 정도로, 아 아테나의 완성은 이거다. 싶었습니다. Zeppelin 설치참고: https://b.luavis.kr/server/zeppelin-with-athena zeppelin과 athena를 연동하는 방법은 위의 포스팅에 아주 정리가 잘 되어 있습니다. Zeppelin 에서 ALB 로그 분석Zeppelin의 설치는 위의 포스팅을 쭉 따라가서 성공하셨을거라고 믿습니다. 그럼 Athena X Zeppelin의 강점을 ALB의 로그를 이리저리 쿼리해 보면서 느껴(?)보도록 합시다. ALB로그는 기본적으로 Apache 로그 포맷에 ALB에서 제공하는 컬럼이 추가된 형태입니다. (Lambda와의 연동 기능이 추가되면서 로그에 컬럼도 추가되었습니다.) 저는 간단하게 ALB에 apache 서버를 물려서 예제 사이트를 구축해놓았습니다. 트래픽은 굳이 일부러 만들진 않고 제 로컬에서만 몇번 들어가보았더니 많이 나오진 않았습니다. 결국 로그가 많이 나오진 않아서 굳이 Athena를 쓰지 않고 Excel 같은 툴로도 충분히 로그를 이리저리 뜯어 볼 수 있었으나, 일반적으로 ALB로그는 양이 많아서 Excel같은 걸로는 뜯어보기가 굉장히 어렵습니다. (버벅댑니다.) 그래서 Athena X Zeppelin을 통해서 간단하게 쿼리하고, 시각화까지 해보도록 하겠습니다. 기본적으로 ALB는 로그를 활성화하면 S3에 저장되고, Athena는 다음과 같은 DDL을 통해 ALB로그를 바라보는 테이블을 생성할 수 있습니다. CREATE EXTERNAL TABLE IF NOT EXISTS alb_logs ( type string, time string, elb string, client_ip string, client_port int, target_ip string, target_port int, request_processing_time double, target_processing_time double, response_processing_time double, elb_status_code string, target_status_code string, received_bytes bigint, sent_bytes bigint, request_verb string, request_url string, request_proto string, user_agent string, ssl_cipher string, ssl_protocol string, target_group_arn string, trace_id string, domain_name string, chosen_cert_arn string, matched_rule_priority string, request_creation_time string, actions_executed string, redirect_url string, lambda_error_reason string, new_field string ) ROW FORMAT SERDE &apos;org.apache.hadoop.hive.serde2.RegexSerDe&apos; WITH SERDEPROPERTIES ( &apos;serialization.format&apos; = &apos;1&apos;, &apos;input.regex&apos; = &apos;([^ ]*) ([^ ]*) ([^ ]*) ([^ ]*):([0-9]*) ([^ ]*)[:-]([0-9]*) ([-.0-9]*) ([-.0-9]*) ([-.0-9]*) (|[-0-9]*) (-|[-0-9]*) ([-0-9]*) ([-0-9]*) \\&quot;([^ ]*) ([^ ]*) (- |[^ ]*)\\&quot; \\&quot;([^\\&quot;]*)\\&quot; ([A-Z0-9-]+) ([A-Za-z0-9.-]*) ([^ ]*) \\&quot;([^\\&quot;]*)\\&quot; \\&quot;([^\\&quot;]*)\\&quot; \\&quot;([^\\&quot;]*)\\&quot; ([-.0-9]*) ([^ ]*) \\&quot;([^\\&quot;]*)\\&quot; \\&quot;([^\\&quot;]*)\\&quot;($| \\&quot;[^ ]*\\&quot;)(.*)&apos;) LOCATION &apos;s3://your-alb-logs-directory/AWSLogs/&lt;ACCOUNT-ID&gt;/elasticloadbalancing/&lt;REGION&gt;&apos;;파티션을 나눠주면 더 좋겠지만, 지금은 일단 데이터 양이 많지 않으므로 생략합니다. 일단 테이블을 만들었으니, 테이블이 어떻게 생겨먹었는지 확인해 봐야겠죠. -- 테이블 예시 select * from &quot;logs&quot;.&quot;alb_logs_apache&quot; limit 10;네, ALB의 컬럼들은 이렇게 생겼습니다. 그럼, 제 가짜 사이트에 요청이 몇번이나 들어왔는지 확인해 볼까요? -- 전체 리퀘스트 수 select count(*) as requests from &quot;logs&quot;.&quot;alb_logs_apache&quot;;그럼, 어떤 분들이 들어왔는지 확인해보도록 하겠습니다. -- 요청수 top 10 client ip select client_ip, count(*) as requests from &quot;logs&quot;.&quot;alb_logs_apache&quot; group by client_ip order by requests desc limit 10;음… 제 가짜 사이트에 관심이 많은 이분들은 그럼 어떤 요청들을 날리는지 볼까요? -- 특정 Client_ip의 요청 URI SELECT request_url FROM &quot;logs&quot;.&quot;alb_logs_apache&quot; WHERE client_ip=&apos;000.000.000.000&apos;역시, 이상한 bot입니다. 워드프레스의 취약점을 노리는 봇들이 많다고 하는데 실체를 이렇게 확인하게 되네요. 그 외에도 응답을 잘 주고 있는지 봐야겠죠? -- 응답 코드 별 count select elb_status_code, count(*) from &quot;logs&quot;.&quot;alb_logs_apache&quot; group by elb_status_code order by count(*) desc;정상적인 사이트라면 200 응답이 많겠지만, 제 사이트에는 200응답 보다는 404 응답이 많네요. 여기서 진짜 좋은점은, Zeppelin에서 파이차트도 간단하게 만들 수 있다는 점입니다. 한눈에 쏙 들어오니 보기가 좋네요. -- target ip 별 count (로드밸런싱이 잘 되고 있는지. ) select target_ip, count(*) from &quot;logs&quot;.&quot;alb_logs_apache&quot; group by target_ip;위처럼 로드밸런싱이 잘 되고 있는지 확인하는것도 간단해 보이지만 중요합니다. 실제로 로드밸런싱이 잘 안되고 있는 사례도 많고, 그런 경우 장애의 포인트가 되기도 때문입니다. -- 시계열 분석... select date_trunc(&apos;hour&apos;,from_iso8601_timestamp(time)) as TIME, count(*) as count from &quot;logs&quot;.&quot;alb_logs_apache&quot; group by TIME order by TIME ASC;시계열 분석을 위해서는 위에 코드 처럼 먼저 String 타입으로 된 시간을 Date 타입으로 변경해주는 작업이 필요합니다. 또한 로그상에는 ms 단위까지 표현이 되지만 시간단위로 잘라줘야 집계가 편합니다. 위와 같이 쿼리하면 시간단위로 내 사이트엔 몇명이 오는지 파악할 수 있고, 제플린에서는 line chart로 바로 시각화가 가능합니다. 아래처럼 대시보드를 만들 수 도 있습니다. Athena X Zeppelin위의 예시처럼, Athena의 아쉬운 인터페이스를 Zeppelin을 통해 보완할 수 있습니다. 설치도 docker를 통해서 간단하게 진행할 수 있고, Zeppelin의 공유 기능을 이용하면 팀원 간에 협업도 가능할 것 같고 좋네요. 개인적으로는 Quicksight 처럼 애매한 BI 툴을 쓰는 것보다는 무료인 Zeppelin을 쓰는 것도 전문적인 BI를 요구하는게 아니라면 괜찮을 것 같습니다.","link":"/2019/05/24/athena-with-zeppelin/"},{"title":"EMR 에 올라간 Zeppelin 활용하여 Athena 이용하기. ","text":"들어가기 전에…최근에 AWS Datascience 소그룹 모임의 자료를 쭉 보던 중 아주 정리가 잘 되어있는 자료를 보았습니다. AWS 기반 지속 가능한 데이터 분석하기 라는 자료인데 SK 빅데이터 허브에서 제공하는 배달 업종 통화 기록을 베이스로 spark로 데이터를 변환하고 athena, Presto, tableau 를 이용해서 시각화 하는 내용을 담고 있습니다. ETL 부터 시각화까지의 내용을 아주 재밌는 데이터와 함께 잘 정리한 글입니다. (AWS 데이터사이언스 모임은 가보지 않았지만 언젠간 한번 꼭 참석하리라 마음을 먹은지 벌써 몇달째… ) 한편, 위의 글에서는 Zeppelin을 Spark의 노트북으로써만 한정해서 사용하는게 조금 아쉬웠달까요. 물론 Tableau라는 훌륭한 시각화 툴을 언급했으므로 굳이 Zeppelin과 Athena를 엮을 필요는 없지만, 전에 글에서도 말했지만 s3+athena의 활용성을 높이기에 Zeppelin 만한 것도 없는것 같습니다. 특히, 위의 글에서 처럼 Spark을 EMR로 관리하고 그 노트북으로써 Zeppelin을 활용하는 환경이라면, Athena에도 접근이 가능하게 하는 것도 활용성을 높여줄 것 같습니다. EMR이 제공하는 Zeppelin 만으로 ETL, 분석을 쉽게할 수 있도록 노트북 환경을 만들고 여차하면 시각화까지 해결하는거죠. (물론 Zeppelin의 시각화에는 한계가 있습니다. Zeppelin은 노트북일 뿐 BI툴이 아니니까요) (이 문단은 안읽으셔도 됩니다. EMR, Zeppelin의 애매한 관계에 대한 넉두리입니다.)사실, Zeppelin을 이런 식으로 꽤나 중요하게 사용하고 싶다면 굳이 EMR에서 제공하는 Zeppelin을 이용하지 않아도 됩니다. EMR은 비싸니까요. EC2위에 직접 Zeppelin을 설치하고 따로 관리를 하는 편이 나을 수도 있습니다. EMR은 사실 항시 유지하는 클러스터를 위한 솔루션이 아닙니다. 반면에 Zeppelin을 멀티유저 환경에서 사용하면 항상 띄어놓는게 맞겠죠. 그래도 24X7 유지하는 클러스터를 위해서 EMR을 사용한다면 Multi-Master 기능을 고려할텐데 심지어 Multi-Master모드로 EMR을 올린다면 Zeppelin을 같이 사용할 수 없습니다. 그러니 조합이 꽤 애매합니다. 이것저것 고려하니 복잡해졌네요. 그냥 AWS에서 EMR 프로비저닝 할때 Athena와의 연계를 미리 고려해서 Zeppelin에 Interpreter 하나만 추가해놔주면 고민이 쉽게 해결될텐데… 이유야 어떻든 간에, 이 글의 목표는 EMR에 올려진 Zeppelin에 Athena를 연결시키는 겁니다. EC2에 Zeppelin이 올라가 있다고 해도 크게 달라지진 않을 것 같습니다. Role을 이용한 권한 연결이 핵심이니까요. 서론이 길었습니다. 얼른 본론으로 갑시다. EMR의 Zeppelin에 Athena Interpreter 추가하기이번 글에서는 EMR에 Spark와 Zeppelin이 올라가 있다고 가정합니다. EMR에서 제공하는 Zeppelin에 Athena를 연결하려면 크게는 다음과 같은 작업을 해야합니다. IAM Role 수정 Zeppelin에 JDBC Interpreter 설치 Athena 용 JDBC 다운로드 Athena에 맞게 JDBC Interpreter 추가 그럼 하나씩 살펴보겠습니다. 1. IAM Role 수정EMR의 Master에는 기본적으로 DynamoDB, Glue, Kinesis, RDS, S3, SNS, SQS 등에 관한 권한이 설정되어 있습니다. (아래 그림 참고) 그런데 이번에는 Athena와 연동을 시켜줄 것이기 때문에, Athena를 컨트롤할 수 있는 권한을 추가해줘야합니다.뭔가 Default는 건들면 안될 것 같으니 다음과 같이 새로 IAM Role을 추가합니다. Role -&gt; Create Role 클릭 이 역할을 사용할 서비스에서 EC2 선택후 [Next: Permission\bs] 클릭 ‘AmazonElasticMapReduceforEC2Role’ 와 ‘AmazonAthenaFullAccess’ 를 찾아 선택 태그는 무시 (원한다면 당연히 입력하셔도 됩니다.) Role 이름에 ‘EMR_EC2_AthenaRole’ 입력 후 [Create Role] 클릭 이제 Role이 완성되었으니 이를 원래 있던 Role대신 넣어줘야 합니다. Zeppelin은 EMR의 마스터 노드에 있으니 Master노드를 찾아 변경해 Role을 변경해줍니다. EMR -&gt; 해당 Cluster 클릭 -&gt; Hardware -&gt; Master 의 ID 클릭 -&gt; EC2 Instance ID 클릭 (EC2 콘솔로 이동하게 됩니다) EC2 콘솔에서 Actions -&gt; Instance Settings -&gt; Attach/Replace IAM Role 클릭 IAM role 에서 ‘EMR_EC2_AthenaRole’ 선택 후 [Apply] 클릭 자, 이제 EMR의 마스터노드에서는 Athena를 마음껏 컨트롤할 수 있게 되었습니다. 그럼 이제 Zeppelin을 세팅해 봅시다. 2. Zeppelin에 JDBC Interpreter 설치이상하게 EMR의 Zeppelin에는 JDBC Interpreter가 설치되어 있지 않습니다. 일단 Spark 만 컨트롤할 수 있게 가볍게 세팅이 되어 있는것 같네요. 그러니 우선 JDBC Interpreter를 설치해 줍시다. 일단, 마스터 노드에 ssh 로 접근해줍니다. (Windows는 putty 등 사용) 1ssh -i {나의pemkey.pem} hadoop@{Masternode EC2 Public DNS}2` 그 다음엔 일단 Zeppelin을 멈추고, 1sudo stop zeppelin2``` 34jdbc interpreter를 설치하고,56```bash7sudo /usr/lib/zeppelin/bin/install-interpreter.sh -n jdbc Zeppelin을 다시 시작해줍니다. 1sudo start zeppelin 참 쉽죠? 3. Athena 용 JDBC 다운로드그럼 이제 Zeppelin이 사용할 Athena용 JDBC 드라이버를 받아봅시다. 우선 jar 파일이 들어갈 경로를 정해봅시다. 저는 /usr/local/jar 로 정했습니다. 1sudo mkdir /usr/local/jar2sudo cd /usr/local/jar wget을 이용해서 jdbc 파일을 받고, 1sudo wget https://s3.amazonaws.com/athena-downloads/drivers/JDBC/SimbaAthenaJDBC_2.0.7/AthenaJDBC42_2.0.7.jar 마지막으로, zeppelin이 실행시킬 수 있게 권한을 바꿔줍니다. 1sudo chmod 755 AthenaJDBC42_2.0.7.jar 4. Athena에 맞게 JDBC Interpreter 추가자, 이제 마지막입니다. JDBC Interpreter만 Athena에 맞게 세팅해서 구성해주면 끝납니다. 우측상단에 있는 Interpreter를 클릭후 Interpreter 창에서 Create를 누릅니다. 그 후 다음과 같이 세팅합니다. Interpreter Name 은 적당히 ‘athena’ 로 정해주고 Interpreter Group 은 JDBC 를 선택합니다. Properties 에서 default.password, default.user는 빈칸으로 default.driver는 com.simba.athena.jdbc.Driver default.url은 다음과 같이 세팅합니다. jdbc:awsathena://athena.ap-northeast-2.amazonaws.com:443;S3OutputLocation=s3://aws-athena-query-results-{계정번호12자리}-ap-northeast-2;Schema=default;AwsCredentialsProviderClass=com.simba.athena.amazonaws.auth.InstanceProfileCredentialsProvider; (서울리전 기준, 나머지 리전의 경우 리전에 맞춰서 세팅해주면 됩니다.) Artifact는 jar파일을 받은 경로를 입력하면 됩니다. /usr/local/jar/AthenaJDBC42_2.0.7.jar [properties 예시] Save를 누르면 드디어! Athena를 사용할 Interpreter가 생성이 됩니다. 테스트그럼 Athena랑 연결이 잘 되었는지 테스트해 봅시다. Notebook-&gt; Create New Note 에서 Default Interpreter를 Athena 로 선택 후 생성합니다. athena에 등록된 테이블에 쿼리를 날려봤습니다. 잘 되네요. (꼭 얄밉게 잘된것만 보여주는 나쁜 사람들이 있습니다.) 정리지금까지 EMR에서 설치해주는 Zeppelin에 Athena를 얹어봤습니다. 이를 통해서 Spark로 ETL해서 S3로 넣고 Athena로 분석 부족하지만 시각화 까지 Zeppelin이라는 하나의 노트북을 환경 통해 좀 편하게 할 수 있지 않을까 해서 포스팅을 써봤습니다. BI를 고려한다면 Zeppelin 안써도 됩니다. BI툴 쓰세요 ㅎㅎㅎ. 그리고 Athena 안쓰고 Spark 내에서 다 ETL, 분석 까지 가능하다면 굳이 Athena와 연결하는데 에너지 쓸 이유는 없습니다. 다만, BigQuery를 필두로 한 SQL을 통한 분석이 요새 대세인것 같기도 하고 실제로 데이터분석가들이 SQL을 많이 사용하는것 같기도 해서 Athena의 사용성이 좋아지길 기대하고 있습니다. 또 Athena와의 연동이 보시다시피 그렇게 어렵지는 않잖아요? 한편으로는 오픈소스 Zeppelin이 더 잘되길 바라는 마음도 있습니다. Zeppelin 화이팅!","link":"/2019/06/29/EMR-Athena-Zepplin/"},{"title":"스파크에 대한 아주 대략적인 정리","text":"빅데이터를 공부해야할 일이 생겨서, 하둡을 먼저 공부했다. 쌩으로 클러스터를 만들고, 설정값을 변경해보고… 그렇게 클러스터를 만드는 일부터 맵리듀스 어플리케이션을 만드는 일까지 공부하는게 쉽지만은 않았다. 여기서 약간 빅데이터의 벽(?)같은걸 1차로 느꼈다. 그러다가 스파크를 만났다. 공부하다 보니 스파크가 그렇게 어렵지도 않고, 그런데 활용성은 굉장히 좋고, 성능도 좋다는 생각이 든다. 여기서 포인트는 그닥 어렵지 않다는 점이다. 자료들이 주로 scala라는 생소한 언어로 설명이 되어있지만, scala가 별로 어렵지 않다. 언어자체가 직관적이면서 간결하다. 그리고 scala로 배우는 스파크도 그닥 어렵지가 않다. 많은 기능을 제공하고 좋은 성능을 제공하지만, 기능들의 추상화가 잘 되어있어서 단순히 api만 호출하면 된다. 이거 사긴데? 쉽고 빠른 빅데이터 처리 엔진이라니… 그 동안 하둡을 공부하면서 느낀 어떤 답답함 같은게 해소되는 느낌이랄까. 맵리듀스를 짜면서 뭔가 병렬처리로 성능이 향상된다는건 알겠는데 코드 작성이 상당히 번거로웠다.(책에는 간단하다고 나와있는데 원리야 간단하지만 코드짜는건 안간단함 ㅇㅇ) 물론 하둡에는 맵리듀스 외에도 빠르게 분석할 수 있는 엔진들이 있지만, 그 중에 스파크가 가장 매력적으로 보인다. 쉽고 빠른 빅데이터 처리엔진 스파크, 그래서 나는 스파크로 빅데이터를 시작하는 것도 좋다고 생각한다. 스파크가 하둡의 모든것을 대체하지는 못하지만, 스파크를 중심으로 빅데이터 처리에 필요한 것들을 배워나가는 것이 효율?이 좋을 것 같다는 생각이다. 스파크의 이러저러한 기능들을 살펴보다보면, 빅데이터 처리라는 단어에서 느껴지는 거리감이 조금은 해소되지 않을까 한다. 나랑 비슷한 생각을 하셨는지 빅데이터 - 스칼라, 스파크로 시작하기라는 책도 공개되어 있다. 아마 앞으로 정리할 내용들이 이 책을 상당부분 참고할 것 같고, 그렇기 때문에 유사할 수도 있다. 그래서 이 글에는 일단 스파크에 대한 전반적인 내용을 담아보려고 한다. 빅데이터 그리고 하둡스파크가 뭔지를 설명하려면 어쩔 수 없이 하둡얘기를 해야한다. 어쩔 수 없는건 아니고 그냥 이게 설명이 편하다. 심지어 스파크 홈페이지에서도 맨 처음 나오는 설명이 하둡보다 100배 빠르다는 설명이다. 하둡보다 100배나 빠르다는데 하둡은 뭘까. 하둡을 얘기하려면 빅데이터를 얘기해야한다. 하둡이 빅데이터 처리를 위한 플랫폼이니까. 빅데이터란 무엇인가…?그렇다고 빅데이터란 무엇인가에 대해서 얘기를 하자니 좀 부담스럽다. 블록체인이 그렇고 인공지능이 그렇듯, 4차산업혁명을 바라보는 시선이 나는 약간 부담스럽다. 관심이 쏠려서 부담스럽기도 하고, 내 설명이 그 기대에 부응하지 못할까봐 부담스럽기도 하고. 빅데이터는 그냥 커다란 데이터이다. 한줄의 레코드도 데이터이고 수억건의 레코드도 데이터인데 여기서 빅데이터는 당연히 후자이다. 그러면 도대체 어느정도 큰 데이터가 빅데이터인가. 데이터는 당연히 그 처리하는 방법론이 데이터의 사이즈에 따라 달라진다. 크지 않은 데이터들은 기존의 RDBMS, 혹은 엑셀 등에서도 충분히 분석과 레포팅이 가능했다. 그런데 사이즈가 커지면 이러한 기존 틀로 분석하기가 어려워진다. 엑셀에 100mb 이상 넘어가는 데이터를 넣어본 사람은 이해할 것이다. 똑같은 구조의 테이블이지만 그냥 사이즈가 크다는 이유만으로 뭔가 다른 처리방법이 필요하다. 즉, 엔지니어 입장에서 빅데이터를 다시 정의하자면 뭔가 다른 처리방법이 필요할만큼 큰 데이터이다. ‘뭔가 다른 처리방법’ =&gt; 하둡그래서 이렇게 커다란 데이터를 처리하기 위한 방법들을 고안해냈고, ‘뭔가 다른 처리방법’ 중 대표적인 것이 하둡 이라는 것이다. 아주 간단히 설명하자면, 단일 컴퓨터에서의 데이터 처리가 버겁고 비싸다는 사실을 깨달은 엔지니어들이 크지 않은 여러 컴퓨터의 자원을 사용해 데이터를 분산저장, 분산처리 할 수 있게끔 만들어놓은 것이 하둡이다. ‘하둡’은 사실 단일 솔루션을 얘기하지 않는다. 하둡이라는 에코시스템은 수집을 위한 솔루션, 저장을 위한 솔루션, 분석을 위한 솔루션, 자원 관리를 위한 솔루션 등이 포함되어 있는 거대한 생태계이다. 이러한 생태계는 공통적으로 Hadoop에서 제공하는 파일시스템(HDFS), 자원관리매니저(YARN), 맵리듀스 등을 사용한다. 즉, 하둡은 저장은 HDFS에 저장하고, 분석작업은 맵리듀스를 통해 수행하고, 이러한 작업들이 사용하는 자원의 관리는 YARN이 맡는다. 이렇게 말하면 간단해 보이지만 각 구성요소는 굉장히 복잡한 내부 아키텍쳐를 갖고있다. 어쨌든, 그 중에서도 맵리듀스 작업은 하둡의 근-본 이면서도 명확한 한계를 갖고있다. 맵리듀스 잡(작업)의 결과물은 hdfs에 저장해야만 하는데 hdfs는 분산파일시스템이기 때문에 저장과 로드에 상당한 비용(시간/자원소모)이 든다. 물론 하둡 생태계의 많은 솔루션들이 이를 상당부분 개선했지만, 근본적으로 필요한 데이터를 디스크에서 매번 가져오고 저장해야 한다는 문제점이 있다. 앞서 말했지만 HDFS는 고가용성을 고려한 분산스토리지이기 때문에 저장시에 복제가 이루어지기도 한다. 또한 파일 하나를 여러 블록에 나누어 저장하기 때문에 이에 대한 메타데이터 관리도 필요하다. 이 과정이 지나야 비로소 파일저장이 되기 때문에, 이러한 작업이 많으면 많을수록 성능에 엄청난 영향을 미친다. 스파크 그럼 이제(드디어) 스파크 얘기를 해보자. 하둡도 빅데이터를 다루기 위한 강력한 에코시스템이지만, 스파크도 하둡을 빠르게 대체하고 있을만큼 강력한 엔진이다. (정확히 말하면 하둡을 대체한다기 보단, 하둡의 분석 어플리케이션을 대체하고 있다.) 스파크가 가진 장점은 다음과 같다. 장점위에서 봤듯이 MapReduce는 디스크에 저장하는 과정이 성능에 영향을 끼친다. 그런데 스파크는 필요한 데이터를 메모리(ram)에 저장해서 이러한 문제를 해결한다. 당연히 디스크보다 메모리가 더 빠르다. 그러니 스파크는 빠르다. 아래 그림을 보자. 스파크는 위 그림처럼 작업이 끝나면 이를 RDD라는 분산 메모리에 저장한다. 이 RDD라는 객체가 바로 스파크의 핵심이다. 이것을 통해 빠른 속도를 내고, 내결함성을 구현하고, 고수준의 api를 제공한다. RDD를 이해하는 것이 스파크를 이해하는 것이다. 여기서는 이것만 기억하자. 스파크는 RDD라는 것을 구현해 메모리 상에서 연산한다. 그래서 빠르다. 스파크는 단순히 빠르기만 한게 아니라, 여러가지 편의성도 제공한다. 빠른데 쓰기가 어렵다면 무슨소용이겠는가 맵리듀스 어플리케이션을 개발하려면 우선 맵퍼와 리듀서라는 것을 따로 구현을 해야하고 이를 통합하는 메인 클래스를 구현해야 한다. 그런데 스파크는 이러한 작업을 위한 고수준 api를 제공한다. 예를들어 filter() 같은 api를 통해 간단하게 데이터를 필터링할 수 있고. map(), flatmap()등을 통해 데이터를 맵핑할 수 있다. 이를 구동시키기 위한 언어도 java, python, scala, R 등으로 그 범위가 넓다. 또한 활용범위도 넓다. Hadoop에도 다양한 컴포넌트가 있듯이 Spark에도 다양한 컴포넌트가 존재한다. SQL, ML, Graph, Streaming 등 다양한 활용이 가능하다. 즉, 빠르고, 쉽고, 활용범위가 넓다. 무슨 말만 들으면 만병통치약 같은데, 사실 이게 ‘맵리듀스에 비해서’ 쉽고 ‘맵리듀스에 비해서’ 빠른 것이다. 어쨌든 스파크가 강력한건 사실이지만, 당연히 단점도 있다. 단점우선 메모리를 사용하기 때문에 상대적으로 비싸다. 데이터를 모두 메모리에서 처리하기 때문에 클러스터의 메모리가 넉넉해야한다. 엄밀한 의미에서의 스트리밍을 지원하지 않는다. 스파크에도 Spark Streaming 이라는게 있어서 스트리밍 데이터 처리를 지원하지만 (꽤 널리 쓰이지만) spark streaming은 micro batch로 구현이 된다. 즉 배치작업을 작게 하는 방식으로 스트리밍 처리를 구현한다. 이게 먹히는 워크로드도 있겠지만 초단위의 분석을 요구하는 워크로드에는 다른 솔루션(flink 등)이 필요하다. 정리스파크는 빅데이터를 입문하기에 적절한 쉽고 빠른 솔루션이다. 데이터를 메모리에 올려놓고 쓰기에 빠르고 이를 복원가능하도록 만들기 위해 RDD라고 불리는 것을 구현했다. 적용할 수 있는 범위도 넓지만 메모리를 쓰기때문에 클러스터에 메모리가 많이 필요하고, 아주 짧은 단위의 스트리밍 분석은 어렵다. 이 정도면 아주 대략적인 정리가 된듯 하다. 다음에는 조금 더 깊숙한 내용을 정리해보겠다. 사실, 스파크의 특징에 대한 내용은 이 글(Why Apache Spark is Fast and How to Make It Run Faster)에 매우 잘 정리가 되어있다. 시간이 된다면 그냥 번역글을 쓰는 것도 큰 도움이 될듯…?","link":"/2019/12/11/what-is-spark/"}],"tags":[{"name":"python","slug":"python","link":"/tags/python/"},{"name":"data","slug":"data","link":"/tags/data/"},{"name":"generate","slug":"generate","link":"/tags/generate/"},{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"jekyll","slug":"jekyll","link":"/tags/jekyll/"},{"name":"AWS","slug":"AWS","link":"/tags/AWS/"},{"name":"SAML","slug":"SAML","link":"/tags/SAML/"},{"name":"aws","slug":"aws","link":"/tags/aws/"},{"name":"rds","slug":"rds","link":"/tags/rds/"},{"name":"readreplica","slug":"readreplica","link":"/tags/readreplica/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"commands","slug":"commands","link":"/tags/commands/"},{"name":"PostgreSQL","slug":"PostgreSQL","link":"/tags/PostgreSQL/"},{"name":"queries","slug":"queries","link":"/tags/queries/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Overlay","slug":"Overlay","link":"/tags/Overlay/"},{"name":"network","slug":"network","link":"/tags/network/"},{"name":"bandwidth","slug":"bandwidth","link":"/tags/bandwidth/"},{"name":"WindowsServer","slug":"WindowsServer","link":"/tags/WindowsServer/"},{"name":"ActiveDirectory","slug":"ActiveDirectory","link":"/tags/ActiveDirectory/"},{"name":"EC2","slug":"EC2","link":"/tags/EC2/"},{"name":"T3","slug":"T3","link":"/tags/T3/"},{"name":"Jekyll","slug":"Jekyll","link":"/tags/Jekyll/"},{"name":"jekyll-swiss","slug":"jekyll-swiss","link":"/tags/jekyll-swiss/"},{"name":"theme","slug":"theme","link":"/tags/theme/"},{"name":"reviewrepublic","slug":"reviewrepublic","link":"/tags/reviewrepublic/"},{"name":"cpr","slug":"cpr","link":"/tags/cpr/"},{"name":"DigitalOcean","slug":"DigitalOcean","link":"/tags/DigitalOcean/"},{"name":"migration","slug":"migration","link":"/tags/migration/"},{"name":"revierepublic","slug":"revierepublic","link":"/tags/revierepublic/"},{"name":"RDS","slug":"RDS","link":"/tags/RDS/"},{"name":"issue","slug":"issue","link":"/tags/issue/"},{"name":"read replica","slug":"read-replica","link":"/tags/read-replica/"},{"name":"bigdata","slug":"bigdata","link":"/tags/bigdata/"},{"name":"zeppelin","slug":"zeppelin","link":"/tags/zeppelin/"},{"name":"athena","slug":"athena","link":"/tags/athena/"},{"name":"cloudfront","slug":"cloudfront","link":"/tags/cloudfront/"},{"name":"log","slug":"log","link":"/tags/log/"},{"name":"elasticsearch","slug":"elasticsearch","link":"/tags/elasticsearch/"},{"name":"storage","slug":"storage","link":"/tags/storage/"},{"name":"lambda","slug":"lambda","link":"/tags/lambda/"},{"name":"concurrency","slug":"concurrency","link":"/tags/concurrency/"},{"name":"lambda@edge","slug":"lambda-edge","link":"/tags/lambda-edge/"},{"name":"serverless","slug":"serverless","link":"/tags/serverless/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"icarus","slug":"icarus","link":"/tags/icarus/"},{"name":"spark","slug":"spark","link":"/tags/spark/"},{"name":"troubleshooting","slug":"troubleshooting","link":"/tags/troubleshooting/"},{"name":"AmazonLinux2","slug":"AmazonLinux2","link":"/tags/AmazonLinux2/"},{"name":"airflow","slug":"airflow","link":"/tags/airflow/"},{"name":"ec2","slug":"ec2","link":"/tags/ec2/"},{"name":"bastionhost","slug":"bastionhost","link":"/tags/bastionhost/"},{"name":"ssh","slug":"ssh","link":"/tags/ssh/"},{"name":"portfowarding","slug":"portfowarding","link":"/tags/portfowarding/"},{"name":"security","slug":"security","link":"/tags/security/"},{"name":"DocumentDB","slug":"DocumentDB","link":"/tags/DocumentDB/"},{"name":"ALB","slug":"ALB","link":"/tags/ALB/"}],"categories":[{"name":"Bigdata","slug":"Bigdata","link":"/categories/Bigdata/"},{"name":"blog","slug":"blog","link":"/categories/blog/"},{"name":"Security","slug":"Security","link":"/categories/Security/"},{"name":"AWS","slug":"AWS","link":"/categories/AWS/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"Database","slug":"Database","link":"/categories/Database/"},{"name":"Container","slug":"Container","link":"/categories/Container/"},{"name":"SAML","slug":"Security/SAML","link":"/categories/Security/SAML/"},{"name":"Computing","slug":"AWS/Computing","link":"/categories/AWS/Computing/"},{"name":"Database","slug":"AWS/Database","link":"/categories/AWS/Database/"},{"name":"Blogging","slug":"Blogging","link":"/categories/Blogging/"},{"name":"Others","slug":"Others","link":"/categories/Others/"},{"name":"PostgreSQL","slug":"Database/PostgreSQL","link":"/categories/Database/PostgreSQL/"},{"name":"Data Analytics","slug":"AWS/Data-Analytics","link":"/categories/AWS/Data-Analytics/"},{"name":"Docker","slug":"Container/Docker","link":"/categories/Container/Docker/"},{"name":"Spark","slug":"Bigdata/Spark","link":"/categories/Bigdata/Spark/"},{"name":"Airflow","slug":"Bigdata/Airflow","link":"/categories/Bigdata/Airflow/"}]}